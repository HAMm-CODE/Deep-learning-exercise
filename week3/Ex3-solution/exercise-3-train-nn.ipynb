{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13649827,"sourceType":"datasetVersion","datasetId":8677139}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution. Please read all the instructions in each notebook carefully.**  ","metadata":{"deletable":false,"editable":false,"slideshow":{"slide_type":""},"tags":[]}},{"cell_type":"markdown","source":"# Exercise 3: Deep Learning Practices\n\nIn this exercise, you will familiarize yourself with various deep learning practices, including hyper-parameter tuning, regularization, and optimization techniques. You will apply these techniques within the context of a binary classification task designed to classify a given audio file as either speech or music. Please refer to the \"ex3_instructions.pdf\" file for a complete description of the problem and the applied techniques.\n\nCode Template\n\nTo complete this assignment, you will progress through four different stages (tasks):\n\n**Task 1. Basic model architecture, training, and testing loops (5 points)**\n\n**Task 2. Fine-tuning practices (5 points)**\n\n**Task 3. Optimization practices (5 points)**\n\n**Task 4. Regularization practices (5 points)**\n\n### **Deliverables:** \n\nPlease submit the completed notebooks below, along with all the requested trained models (.pth) for the corresponding subtasks.\n\n* ex3_train_NN.ipynb\n* 'base_model.pth'\n* 'relu_model.pth'\n* 'lr_model.pth'\n* 'shuffle_model.pth'\n* 'bs_model.pth'\n* 'SGD_model.pth'\n* 'normalized_model.pth'\n* 'pooled_model.pth'\n\n### **Data**\n\nThe dataset used for this exercise consists of a collection of audio .wav files, each with a duration of 5 seconds. You can find and download the data from the Moodle page of the course (Exercise 3). The dataset is provided as a ~150 MB ZIP file on Moodle. Please download the data and extract it into the same folder as the exercise files, naming the folder \"dataset_ex3.\" The \"dataset_ex3\" folder includes \"speech_wav\" and \"music_wav\" folders, each containing audio files for speech and music, respectively.\n\n*Note:* Your dataset path should point to the \"dataset_ex3\" folder, which contains the \"speech\" and \"music\" sub-folders. Be mindful of any extra folder levels that may be created when extracting the \"dataset_ex3.zip\" file.\n\nAfter downloading the data and setting up the folders, you are ready to begin the exercise tasks. Let's get started!","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"d94c2340653a0870a8d5f6ec944bc204","grade":false,"grade_id":"cell-5ed66fa5f1209cda","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"markdown","source":"## Task 1: Basic Model Design and Training (5 Points)\n\nIn this task, you will design and train a basic model for a speech versus music audio classification task. The model takes an audio signal as input and outputs a class label predicting whether the audio signal is speech or music. This exercise uses a convolutional neural network model.\n\nYour task is to design the basic block of the neural model, write a training loop function to train the model for several epochs, and implement a validation function to test the model's performance on the validation set.\n\n### Summary of Tasks for This Stage\n\n**Task 1.1: Design the model architecture** (2 points)\n\n**Task 1.2 & Task 1.3: Complete the training and validation loops** (2 points)\n\n**Successful run of the submitted 'base_model.pth'** (1 point)\n\n### Deliverables from this task:\n\n* 'base_model.pth'\n\n**Important**: Always use deep copy if you want to make your own config dictionary in the code. The exercise has already separate config placeholder for each task but you if you want to test with different config then make sure to use deep copy instead of direct initialization to another variable","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ceceb9f536e4f43322186ce360d5e2bc","grade":false,"grade_id":"cell-45f1c2af76b09749","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"skip_training = False   # You can set it to True if you want to run inference on your trained model. ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.400980Z","iopub.execute_input":"2025-11-10T20:19:10.401378Z","iopub.status.idle":"2025-11-10T20:19:10.416372Z","shell.execute_reply.started":"2025-11-10T20:19:10.401359Z","shell.execute_reply":"2025-11-10T20:19:10.415428Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"af5a674227d8d2e73b157fd76732f431","grade":true,"grade_id":"cell-7b0f342d5ac2e1ee","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.417139Z","iopub.execute_input":"2025-11-10T20:19:10.417354Z","iopub.status.idle":"2025-11-10T20:19:10.430406Z","shell.execute_reply.started":"2025-11-10T20:19:10.417326Z","shell.execute_reply":"2025-11-10T20:19:10.429772Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"Import all necessary libraries.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"5f7fb51763c008a67fe74c8129b45fc5","grade":false,"grade_id":"cell-e350d1b0e2c20bc0","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torchaudio\nimport glob\nimport os\nimport numpy as np\nimport time\nimport random\nimport matplotlib.pyplot as plt\nimport copy\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set random seeds for all libraries\nrandom.seed(1)\nnp.random.seed(1)\ntorch.manual_seed(1)\ntorch.cuda.manual_seed(1)\ntorch.cuda.manual_seed_all(1) \n\n# Ensure deterministic behavior\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.431212Z","iopub.execute_input":"2025-11-10T20:19:10.431451Z","iopub.status.idle":"2025-11-10T20:19:10.450007Z","shell.execute_reply.started":"2025-11-10T20:19:10.431424Z","shell.execute_reply":"2025-11-10T20:19:10.449273Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"Select the device","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9b3657c9847477acc023c07d46137dbd","grade":false,"grade_id":"cell-8b4b189ba6aee857","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"if torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7f8d25bdf07ddc1ec85c4f23e98397dd","grade":false,"grade_id":"cell-3a10875707170318","locked":true,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.451780Z","iopub.execute_input":"2025-11-10T20:19:10.451986Z","iopub.status.idle":"2025-11-10T20:19:10.457631Z","shell.execute_reply.started":"2025-11-10T20:19:10.451970Z","shell.execute_reply":"2025-11-10T20:19:10.456857Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"Add the data path","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"8ef7495bbf3dfca15975d6437209e0a4","grade":false,"grade_id":"cell-417bbd087fc67322","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"path = \"/kaggle/input/dataset-ex3\" # you can change the path if you want to store the dataset somewhere else.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.458548Z","iopub.execute_input":"2025-11-10T20:19:10.459134Z","iopub.status.idle":"2025-11-10T20:19:10.471522Z","shell.execute_reply.started":"2025-11-10T20:19:10.459116Z","shell.execute_reply":"2025-11-10T20:19:10.470909Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b13c030978f3f075350c78ac589f2cb6","grade":true,"grade_id":"cell-7a05e8ea3f9468d7","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.472336Z","iopub.execute_input":"2025-11-10T20:19:10.472671Z","iopub.status.idle":"2025-11-10T20:19:10.486070Z","shell.execute_reply.started":"2025-11-10T20:19:10.472648Z","shell.execute_reply":"2025-11-10T20:19:10.485459Z"}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"The cell below defines a function for preparing a list of audio file paths from specified directories. It reads .wav files for each class (music and speech), shuffles the data, and splits it into training and validation sets based on the given validation split ratio.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"55b06875bab3a9f774bf971c76c56be9","grade":false,"grade_id":"cell-f7d981238aed90fa","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"class DataGenerator(Dataset):\n    def __init__(self, mode):\n        super(DataGenerator, self).__init__()\n        self.dataset_path = path\n        \n        self.duration_per_file_in_s = 5 # crop the input audios to 5 seconds\n        self.sampling_rate=22050\n        self.samples_per_file = int(self.duration_per_file_in_s * self.sampling_rate)\n        \n        self.music_files = glob.glob(os.path.join(path, 'music_wav', '*.wav'))\n        self.speech_files = glob.glob(os.path.join(path, 'speech_wav', '*.wav'))\n\n        # Splitting files for train and test\n        music_split_idx = int(0.7 * len(self.music_files))\n        speech_split_idx = int(0.7 * len(self.speech_files))\n        \n        if mode == 'train':\n            self.music_files = self.music_files[:music_split_idx]\n            self.speech_files = self.speech_files[:speech_split_idx]\n        elif mode == 'test':\n            self.music_files = self.music_files[music_split_idx:]\n            self.speech_files = self.speech_files[speech_split_idx:]\n\n        # Combine all files for indexing\n        self.files = self.music_files + self.speech_files\n        self.labels = [0]*len(self.music_files) + [1]*len(self.speech_files)\n\n    def __getitem__(self, item):\n        \n        file_path = self.files[item]\n        label = self.labels[item]\n\n        audio_data, sr = torchaudio.load(file_path)  # Returns a tensor with shape [channels, samples]\n        \n        # Resample if the sample rate is not 22050\n        if sr !=  self.sampling_rate:\n            resample_transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq= self.sampling_rate)\n            audio_data = resample_transform(audio_data)\n\n        if audio_data.size(0) > 1:  # Check if more than 1 channel\n            audio_data = torch.mean(audio_data, dim=0, keepdim=True)\n\n        if audio_data.size(1) > self.samples_per_file:\n            audio_data = audio_data[:, :self.samples_per_file]  # Truncate\n        else:\n            pad_length = self.samples_per_file - audio_data.size(1)\n            audio_data = torch.nn.functional.pad(audio_data, (0, pad_length))  # Pad at the end\n\n\n        return audio_data, label\n\n    def __len__(self):\n        return len(self.files)","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2a244a21c32aaa4b68afc50678b55ac3","grade":false,"grade_id":"cell-6873f2001d3f2c75","locked":true,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.486781Z","iopub.execute_input":"2025-11-10T20:19:10.487012Z","iopub.status.idle":"2025-11-10T20:19:10.502909Z","shell.execute_reply.started":"2025-11-10T20:19:10.486991Z","shell.execute_reply":"2025-11-10T20:19:10.502256Z"}},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"### Task 1.1: Model Architecture\n\nIn this task, you will design a **configurable convolutional neural network** and implement the model class based on the given instructions.  \n\nIn the **`MyModel`** template below, which is designed to define the model class, fill in the blanks to complete the architecture according to the provided configuration dictionary.  \n\nThe **`BasicBlock`** class serves as a fundamental building block for constructing convolutional networks. Each block consists of:  \n- A convolutional layer (`Conv1d`)  \n- An optional batch normalization layer  \n- A non-linear activation function (`ReLU` or `Tanh`)  \n- An optional dropout layer  \n\nAt the end of the network, you will include a **Global Average Pooling** layer, a **Flatten** layer, and an **Output Activation (Sigmoid)** layer.  \n\nYou are also given a configuration dictionary (`base_config`) that defines model hyperparameters, such as the number of blocks, channels, and layer properties:\n\n```python\nbase_config = {\n    'nb_basic_blocks': int,\n    'conv_channels': list,\n    'kernel_size': int,\n    'stride': int,\n    'non_linearity': str,\n    'use_batchnorm': bool,\n    'use_dropout': bool,\n    'batch_size': int,\n    'shuffle': bool,\n    'optimizer_type': str,\n    'learning_rate': float,\n    'apply_pooling': bool\n}\n```\n**Hint:**  \nYou will need to use a **loop** to create multiple `BasicBlocks` dynamically and append it into layers.  \nUse `nn.ModuleList()` to store these blocks so that PyTorch registers them as part of the model.\n\n<div style=\"text-align: center;\">\n    <figure>\n        <img src=\"neural_network_model.jpg\" alt=\"Model Architecture\" style=\"width:900px; height:auto;\"/>\n    </figure>\n</div>\n\nuseful links:\n* https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n* https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html\n* https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html\n* https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"95fc72523e6ac00625032679215b3085","grade":false,"grade_id":"cell-c05d98e2f3351f61","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, non_linearity, apply_batchnorm, apply_dropout):\n        super().__init__()\n        \n        self.apply_batchnorm = apply_batchnorm\n        self.apply_dropout= apply_dropout\n        \n        self.conv_layer = nn.Conv1d(in_channels=in_channels, out_channels=out_channels,kernel_size=kernel_size, stride=stride)\n\n        if apply_batchnorm:\n            self.bn = nn.BatchNorm1d(out_channels)\n            \n        if non_linearity == \"ReLU\":\n            self.activation_fn = nn.ReLU()\n        elif non_linearity == \"Tanh\":\n            self.activation_fn = nn.Tanh()\n\n        if apply_dropout:\n            self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = self.conv_layer(x)\n        if self.apply_batchnorm:\n            x = self.bn(x)     \n        x = self.activation_fn(x)  \n      \n        if self.apply_dropout:\n            x=self.dropout(x)\n        return x\n\n\nclass MyModel(nn.Module):\n    def __init__(self, nb_basic_blocks, conv_channels, kernel_size, stride, non_linearity, apply_batchnorm, apply_dropout, apply_pooling):\n        super().__init__()\n        self.apply_pooling = apply_pooling\n        self.feature_extractor = None\n\n        self.layers = nn.ModuleList()\n\n        # your code here for initializing layers\n        # ---------------------------------------------------------------------\n        # In this task, you will design a flexible CNN model using the BasicBlock\n        # defined above. Each BasicBlock may include:\n        #   - Conv1d layer\n        #   - Optional BatchNorm1d\n        #   - Activation (ReLU or Tanh)\n        #   - Optional Dropout\n        #\n        # Follow the steps below to build the model:\n        #\n        # 1. nn.ModuleList() is initialized and stored in `self.layers`.\n        #\n        # 2. Use a loop to create the specified number of BasicBlocks:\n        #       - For the first block, set in_channels = 1\n        #       - For the remaining blocks, set in_channels = conv_channels[i-1]\n        #       - Set out_channels = conv_channels[i]\n        #       - Pass kernel_size, stride, non_linearity, apply_batchnorm,\n        #         and apply_dropout as parameters to BasicBlock.\n        #       - Append each BasicBlock to `self.layers`.\n        #\n        # 3. If apply_pooling = True:\n        #       - After each BasicBlock in the loop iteration (except the last one),\n        #         append a MaxPool1d layer to `self.layers`\n        #         with the same kernel_size and stride.\n        #\n        # 4. Once all layers are added, wrap them using nn.Sequential:\n        #       self.feature_extractor = nn.Sequential(*self.layers)\n        #\n        # 5. Add the remaining layers:\n        #       - A Global Average Pooling layer (AdaptiveAvgPool1d)\n        #       - A Flatten layer\n        #       - If apply_pooling = True, also include a Linear (fc) layer\n        #         that maps the last channel size to 1.\n\n        in_channels = 1\n        for i in range(nb_basic_blocks):\n            out_channels = conv_channels[i]\n            \n            # Create block\n            block = BasicBlock(in_channels, out_channels, kernel_size, stride, \n                               non_linearity, apply_batchnorm, apply_dropout)\n            self.layers.append(block)\n    \n            # Add pooling after each block except the last one\n            if self.apply_pooling and i < nb_basic_blocks - 1:\n                self.layers.append(nn.MaxPool1d(kernel_size=kernel_size, stride=stride))\n            \n            # Update for next block\n            in_channels = out_channels\n    \n            # Wrap layers in a Sequential container\n            self.feature_extractor = nn.Sequential(*self.layers)\n    \n            # Global Average Pooling, Flatten, FC, Sigmoid\n            self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n            self.flatten = nn.Flatten()\n    \n            if self.apply_pooling:\n                # Output channel = last value in conv_channels\n                self.fc = nn.Linear(conv_channels[-1], 1)\n    \n            self.output_activation = nn.Sigmoid()\n\n    def forward(self, x):\n        # your code here for the forward pass\n        # ---------------------------------------------------------------------\n        # 1. Pass the input through the feature extractor (self.feature_extractor),\n        #    which consists of multiple BasicBlocks and optional pooling layers.\n        #\n        # 2. Apply the global average pooling layer to reduce the spatial dimension.\n        #\n        # 3. Flatten the pooled output to a vector using the flatten layer.\n        #\n        # 4. If apply_pooling = True, pass the output through the fully connected layer.\n        #\n        # 5. Finally, apply the Sigmoid activation to obtain the final output.\n        # ---------------------------------------------------------------------\n        #\n        # Hint:\n        # The order should always be:\n        # feature_extractor → global_avg_pool → flatten → (fc) → sigmoid\n        \n         # 1. Feature extraction\n        x = self.feature_extractor(x)\n\n        # 2. Global Average Pooling\n        x = self.global_avg_pool(x)\n\n        # 3. Flatten\n        x = self.flatten(x)\n\n        # 4. Fully connected (if pooling applied)\n        if self.apply_pooling:\n            x = self.fc(x)\n\n        # 5. Output activation\n        x = self.output_activation(x)\n        return x\n\ndef get_num_trainable_parameters(model):\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f'The model has {num_params} trainable parameters.')\n    return num_params","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"88aab1abbbcdec9066626e68373a8d3b","grade":false,"grade_id":"cell-30eb42e554cad534","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.504765Z","iopub.execute_input":"2025-11-10T20:19:10.504976Z","iopub.status.idle":"2025-11-10T20:19:10.526860Z","shell.execute_reply.started":"2025-11-10T20:19:10.504954Z","shell.execute_reply":"2025-11-10T20:19:10.526226Z"}},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":"### Task 1.2: Define Loss Function and Optimizer\n\nIn this task, you will define a helper function named **`loss_and_optimizer`** that initializes both the **loss function** and the **optimizer** for your model training.\n\nThe function should accept the following parameters:\n- **`model`** – the neural network whose parameters need to be optimized  \n- **`optimizer_type`** – a string specifying the optimizer to use (e.g., `'Adam'` or `'SGD'`)  \n- **`learning_rate`** – a floating-point value specifying the optimizer’s learning rate  \n","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"8b29819f1e3d0f81180cc7caeb0e78a2","grade":false,"grade_id":"cell-6b674c8d5b57ed13","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"def loss_and_optimizer(model, optimizer_type, learning_rate):\n\n    # your code here\n    # 1. Define the loss function as Binary Cross-Entropy Loss.\n    # 2. Initialize the optimizer based on optimizer_type:\n    #    - If 'Adam': use Adam optimizer.\n    #    - If 'SGD': use SGD optimizer.\n    # 3. Return both the criterion and optimizer.\\\n    # 1. Define the loss function\n    criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n    \n    # 2. Initialize the optimizer based on the given type\n    if optimizer_type == 'Adam':\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer_type == 'SGD':\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    else:\n        raise ValueError(f\"Unsupported optimizer type: {optimizer_type}. Choose 'Adam' or 'SGD'.\")\n    \n    # 3. Return both\n    return criterion, optimizer\n    ","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"ebef7344f2f54fd27b69e4465feeea00","grade":false,"grade_id":"cell-08b0c9a4997eb798","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.527625Z","iopub.execute_input":"2025-11-10T20:19:10.527816Z","iopub.status.idle":"2025-11-10T20:19:10.545566Z","shell.execute_reply.started":"2025-11-10T20:19:10.527801Z","shell.execute_reply":"2025-11-10T20:19:10.544806Z"}},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":"### Train and Validate Base Model\n \nIn this task you are required to complete the python dictionary for the neural network configuration. You need to fill the value for the following keys\n\n* conv_channels = 3\n  - First channel has 1 input layer and 32 output layers\n  - Second channel has 32 input layers and 32 output layers\n  - Third channel has 32 input layers and 1 output layer\n  **Remember**: The `conv_channels` key in base_config is a list that holds the the input and output channel layers as parameters for your implementation in **MyModel** class\n* optimizer_type = Adam,\n* learning_rate = 0.0001","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c2ec844ea1798013c10562e44d01d1ea","grade":false,"grade_id":"cell-06a29833e043fcef","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"base_config = {\n    'nb_basic_blocks': 3,\n    'conv_channels': [32, 32, 1],\n    'kernel_size': 11,\n    'stride': 5,\n    'non_linearity': 'Tanh',\n    'use_batchnorm': False,\n    'use_dropout': False,\n    'batch_size': 2,\n    'shuffle': False,\n    'optimizer_type': 'Adam',\n    'learning_rate': 0.0001,\n    'apply_pooling': False\n}","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3bfc778a694212af766bb311aac7b140","grade":false,"grade_id":"cell-a478003cadbc62d8","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.546350Z","iopub.execute_input":"2025-11-10T20:19:10.546570Z","iopub.status.idle":"2025-11-10T20:19:10.564403Z","shell.execute_reply.started":"2025-11-10T20:19:10.546554Z","shell.execute_reply":"2025-11-10T20:19:10.563866Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"Run the cell below to verify the correctness of your solution for the model architecture.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"f5704998fc05af02a715192415970e15","grade":false,"grade_id":"cell-31460a27459e766e","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# Visible tests here\n\nbase_test_config = copy.deepcopy(base_config)\n\nall_tests_successful = True\nmodel = MyModel(base_test_config[\"nb_basic_blocks\"], base_test_config[\"conv_channels\"], base_test_config[\"kernel_size\"], \n                base_test_config[\"stride\"], base_test_config[\"non_linearity\"], base_test_config[\"use_batchnorm\"], \n                base_test_config[\"use_dropout\"], base_test_config[\"apply_pooling\"]).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\ndummy_output = model(dummy_input)\n\n# Test the number of Conv1d layers\nconv1d_count = sum(1 for layer in model.modules() if isinstance(layer, nn.Conv1d))\nif conv1d_count != 3:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 1.2, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n\n# Check the output shape\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 1.2, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n\n# Chech the number of trainable parameters\nnum_params = get_num_trainable_parameters(model)\nexpected_num_parameters = 12033\nif num_params != expected_num_parameters:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 1.2, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n\nif all_tests_successful: \n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")\n","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"59b1bd2597cbd465336a63d3ea29e877","grade":true,"grade_id":"cell-87ae909b7908359a","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.565216Z","iopub.execute_input":"2025-11-10T20:19:10.565437Z","iopub.status.idle":"2025-11-10T20:19:10.589295Z","shell.execute_reply.started":"2025-11-10T20:19:10.565422Z","shell.execute_reply":"2025-11-10T20:19:10.588545Z"}},"outputs":[{"name":"stdout","text":"The model has 12033 trainable parameters.\n\u001b[92mGood job! All visible tests passed! You can proceed further.\u001b[0m\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"def training_loop(nb_epochs, model, optimizer, loss_fn, train_dataloader, test_dataloader, verbose=True):\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n    \n    for epoch in range(1, nb_epochs + 1):\n        model.train()\n        start = time.time()\n        train_loss, correct_predictions = 0., 0.\n        num_samples = 0\n        for i, (input_batch, target_batch) in enumerate(train_dataloader):\n            # your code here for minibatch training\n            # 1. call batch data and labels and set them to the correct device\n            # 2. make the prediction on the data\n            # 3. calculate loss\n            # 4. set optimizer to zero grad\n            # 5. do backward pass\n            # 6. move the optimizer one step forward\n            #  1️⃣ Move batch data and labels to device\n            input_batch = input_batch.to(device)\n            target_batch = target_batch.to(device).view(-1, 1).float()\n\n            # 2️⃣ Forward pass (prediction)\n            predictions = model(input_batch)\n\n            # 3️⃣ Compute loss\n            loss_train = loss_fn(predictions, target_batch)\n\n            # 4️⃣ Zero the gradients before backward pass\n            optimizer.zero_grad()\n\n            # 5️⃣ Backpropagation\n            loss_train.backward()\n\n            # 6️⃣ Update model parameters\n            optimizer.step()\n            \n            # accumulate correct prediction\n            correct_predictions += ((predictions.detach() >= 0.5).int() == target_batch.int()).sum().item() # number of correct predictions\n            train_loss += loss_train.item()\n\n        average_train_loss = train_loss/(i+1)\n        average_train_accuracy = correct_predictions/len(train_dataloader.dataset)\n\n        test_loss, test_accuracy = testing_loop(model, loss_fn, test_dataloader)\n\n        train_losses.append(average_train_loss)\n        val_losses.append(test_loss)\n        train_accuracies.append(average_train_accuracy)\n        val_accuracies.append(test_accuracy)\n\n        end = time.time()\n        epoch_time = round(end - start, 2)\n        if verbose:\n            print(f'Epoch {epoch}, train_loss {average_train_loss:.2f}, train_accuracy: {average_train_accuracy:.4f},',\n                  f'test_loss {test_loss:.2f}, test_accuracy: {test_accuracy:.4f}, time = {epoch_time}')\n\n    return train_losses, val_losses, train_accuracies, val_accuracies","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c2c439226d7fb381b3177f8e313a3369","grade":false,"grade_id":"cell-de7de1fd3d3515a8","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.590143Z","iopub.execute_input":"2025-11-10T20:19:10.590392Z","iopub.status.idle":"2025-11-10T20:19:10.606806Z","shell.execute_reply.started":"2025-11-10T20:19:10.590377Z","shell.execute_reply":"2025-11-10T20:19:10.606064Z"}},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":"### Task 1.3: Training and Validation Loops\n\nIn this task, you will complete the training and validation loops by filling in the blanks according to the provided instructions. The training loop iterates over the dataset to train the model, while the validation loop evaluates the model's performance on a separate validation dataset. The validation loop is called within the training loop to assess the model's accuracy and loss after each training epoch.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9dfab269b47495625a1c4cc2af759869","grade":false,"grade_id":"cell-0a5888fc734d0fea","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"def testing_loop(model, loss_fn, test_dataloader):\n    model.eval()\n    with torch.no_grad():\n        total_loss, correct_predictions = 0., 0.\n        for i, (input_batch, target_batch) in enumerate(test_dataloader):\n            # your code here for minibatch validation\n            # 1. set input_batch, target_batch to correct device\n            # 2. make the prediction on input_batch\n            # 3. calculate loss and add it to previous loss\n            # 4. obtain predicted class labels from predictions\n            # Move input and target to device\n            input_batch = input_batch.to(device)\n            target_batch = target_batch.to(device).view(-1, 1).float()\n\n            #Forward pass (make predictions)\n            predictions = model(input_batch)\n\n            #Compute loss and accumulate\n            loss = loss_fn(predictions, target_batch)\n            total_loss += loss.item()\n            \n            correct_predictions += ((predictions.detach() >= 0.5).int() == target_batch.int()).sum().item()\n            \n    # Average for all batches\n    average_loss = total_loss / (i + 1)  # Use i + 1 for the total number of batches\n    average_accuracy = correct_predictions / len(test_dataloader.dataset)  \n    \n    return average_loss, average_accuracy","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3281f081d681fa8db347f3cbffc916b8","grade":false,"grade_id":"cell-45c3b61513ffd3c8","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.607590Z","iopub.execute_input":"2025-11-10T20:19:10.607779Z","iopub.status.idle":"2025-11-10T20:19:10.625503Z","shell.execute_reply.started":"2025-11-10T20:19:10.607764Z","shell.execute_reply":"2025-11-10T20:19:10.624775Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"def plot_loss_accuracy(train_losses, val_losses, train_acc, val_acc, nb_epochs):\n    epochs = range(1, nb_epochs + 1)\n    best_val_acc = max(val_acc)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # ---- Loss plot ----\n    ax1.plot(epochs, train_losses, label='Train', color='tab:blue')\n    ax1.plot(epochs, val_losses, label='Validation', color='tab:orange')\n    ax1.set_title('Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True)\n\n    # ---- Accuracy plot ----\n    ax2.plot(epochs, train_acc, label='Train', color='tab:blue')\n    ax2.plot(epochs, val_acc, label='Validation', color='tab:orange')\n    ax2.set_title(f'Accuracy (Best: {best_val_acc:.2f})')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n    ax2.grid(True)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"14deb6b2a89c3f3d48d9860781691364","grade":false,"grade_id":"cell-80f4807eea0fd726","locked":true,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.626328Z","iopub.execute_input":"2025-11-10T20:19:10.626541Z","iopub.status.idle":"2025-11-10T20:19:10.642719Z","shell.execute_reply.started":"2025-11-10T20:19:10.626525Z","shell.execute_reply":"2025-11-10T20:19:10.641893Z"}},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":"Run the below cell to check the correctness of your solution for the training loop.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"488ed8b2fdbe9c155b0440e29df1bada","grade":false,"grade_id":"cell-faa5efb13a01620c","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# This cell tests the training code\nall_tests_successful = True\n\nmodel = MyModel(base_test_config[\"nb_basic_blocks\"], base_test_config[\"conv_channels\"], base_test_config[\"kernel_size\"], \n                base_test_config[\"stride\"], base_test_config[\"non_linearity\"], base_test_config[\"use_batchnorm\"], \n                base_test_config[\"use_dropout\"], base_test_config[\"apply_pooling\"]).to(device)\ndummy_optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\ndummy_loss_fn = nn.BCELoss().to(device)\n\n# Create TensorDataset and F\ndummy_inputs = torch.rand(2, 1, 22000).to(device)  \ndummy_labels = torch.randint(0, 2, (2,)).to(device) \n\ndummy_dataset = torch.utils.data.TensorDataset(dummy_inputs, dummy_labels)\ndummy_dl_train = torch.utils.data.DataLoader(dummy_dataset, batch_size=2)  \ndummy_dl_val = torch.utils.data.DataLoader(dummy_dataset, batch_size=8)\n\nfrom unittest.mock import patch, MagicMock\n\nwith patch('torch.Tensor.backward') as mock_backward, patch.object(dummy_optim, 'step') as mock_step:\n    training_loop(\n        nb_epochs=1,\n        model=model,\n        optimizer=dummy_optim,\n        loss_fn=dummy_loss_fn,\n        train_dataloader=dummy_dl_train,\n        test_dataloader=dummy_dl_val,\n        verbose=False\n    )\n    \n    if mock_backward.called:   # check if .backward() was called\n        pass\n    else:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 1.3, Visible test: You forgot to calculate the gradients.\")\n        raise AssertionError(\"You forgot to calculate the gradients.\")\n\n    if mock_step.called: # check if .step() is used\n        pass\n    else:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 1.3, Visible test: You forgot to update the weights.\")\n        raise AssertionError(\"You forgot to update the weights.\")\n    \nif all_tests_successful:\n    success_str = 'Good job! you can now proceed to train your model.'\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5859b2b2df0b5168166474e81a31acd1","grade":true,"grade_id":"cell-c9706ace9d6f2b1c","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.643584Z","iopub.execute_input":"2025-11-10T20:19:10.644015Z","iopub.status.idle":"2025-11-10T20:19:10.669879Z","shell.execute_reply.started":"2025-11-10T20:19:10.643989Z","shell.execute_reply":"2025-11-10T20:19:10.669260Z"}},"outputs":[{"name":"stdout","text":"\u001b[92mGood job! you can now proceed to train your model.\u001b[0m\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"def train_and_test_your_model(config, model_name, save_model):\n\n    train_dataset = DataGenerator(mode='train')\n    test_dataset = DataGenerator(mode='test')\n\n    cpu_generator = torch.Generator(device='cuda')\n    \n    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=config['shuffle'], generator=cpu_generator)\n    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=config['shuffle'], generator=cpu_generator)\n\n    model = MyModel(config['nb_basic_blocks'], config['conv_channels'], config['kernel_size'], config['stride'],\n                    config['non_linearity'], config['use_batchnorm'], config['use_dropout'], config['apply_pooling'])\n    loss_fn, optimizer = loss_and_optimizer(model, config['optimizer_type'], config['learning_rate'])\n\n    model = model.to(device)\n    loss_fn=loss_fn.to(device)\n\n    nb_epochs = 100\n\n    if not skip_training:\n        train_losses, val_losses, train_acc, val_acc = training_loop(nb_epochs, model, optimizer, loss_fn, train_loader, test_loader)\n        if save_model:\n            torch.save(model.state_dict(), f'{model_name}.pth')\n            print(\"Your trained model is saved successfully!\")\n\n        plot_loss_accuracy(train_losses, val_losses, train_acc, val_acc, nb_epochs)\n        results = {\n            \"metrics\": {\n              \"train_losses\": train_losses,\n              \"validation_losses\": val_losses,\n              \"training_accuracy\": train_acc,\n              \"validation_accuracy\": val_acc\n            },\n            \"test_loader\": test_loader,\n            \"loss_fn\": loss_fn\n\n        }\n        return results\n    else:\n        model.load_state_dict(torch.load(f'{model_name}.pth', map_location=device))\n        print(\"Loaded weights from your saved model successfully!\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"97c2372511c0291b326968d3a7c86a3f","grade":false,"grade_id":"cell-be3857f66f0a3bde","locked":true,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.672975Z","iopub.execute_input":"2025-11-10T20:19:10.673178Z","iopub.status.idle":"2025-11-10T20:19:10.680407Z","shell.execute_reply.started":"2025-11-10T20:19:10.673163Z","shell.execute_reply":"2025-11-10T20:19:10.679700Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.681266Z","iopub.execute_input":"2025-11-10T20:19:10.681806Z","iopub.status.idle":"2025-11-10T20:19:10.699803Z","shell.execute_reply.started":"2025-11-10T20:19:10.681788Z","shell.execute_reply":"2025-11-10T20:19:10.698979Z"}},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"### Train the Model\n \nWe will refer to this model as the \"base_model.\" The code will save the model. You are required to submit the trained \"basic_model.pth\"","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"b9cd4deeda5fdfa9675250ec7048b7a4","grade":false,"grade_id":"cell-632afee7f06bac0c","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"results = train_and_test_your_model(base_config, 'base_model', True)","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8b2cef2172f446c60876ebcbbbb5c768","grade":false,"grade_id":"cell-69974db3a8ac874b","locked":true,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T20:19:10.700658Z","iopub.execute_input":"2025-11-10T20:19:10.700854Z","execution_failed":"2025-11-10T20:21:27.437Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, train_loss 0.75, train_accuracy: 0.5000, test_loss 0.70, test_accuracy: 0.5000, time = 2.57\nEpoch 2, train_loss 0.71, train_accuracy: 0.5000, test_loss 0.70, test_accuracy: 0.5000, time = 2.58\nEpoch 3, train_loss 0.71, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.59\nEpoch 4, train_loss 0.71, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.57\nEpoch 5, train_loss 0.71, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.61\nEpoch 6, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 7, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 8, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.59\nEpoch 9, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.52\nEpoch 10, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.53\nEpoch 11, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.59\nEpoch 12, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.63\nEpoch 13, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.56\nEpoch 14, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.56\nEpoch 15, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.55\nEpoch 16, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.59\nEpoch 17, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.56\nEpoch 18, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.53\nEpoch 19, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 20, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.55\nEpoch 21, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 22, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.53\nEpoch 23, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 24, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.56\nEpoch 25, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.52\nEpoch 26, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.52\nEpoch 27, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.73\nEpoch 28, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.62\nEpoch 29, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.6\nEpoch 30, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 31, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 32, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.57\nEpoch 33, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.56\nEpoch 34, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.55\nEpoch 35, train_loss 0.70, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.56\nEpoch 36, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.59\nEpoch 37, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.53\nEpoch 38, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.52\nEpoch 39, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 40, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.57\nEpoch 41, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.52\nEpoch 42, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.53\nEpoch 43, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.57\nEpoch 44, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 45, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.53\nEpoch 46, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.52\nEpoch 47, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.55\nEpoch 48, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.55\nEpoch 49, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.54\nEpoch 50, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.62\nEpoch 51, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.59\nEpoch 52, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.58\nEpoch 53, train_loss 0.69, train_accuracy: 0.5000, test_loss 0.69, test_accuracy: 0.5000, time = 2.57\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Run the below cell to check correctness of your solution for the training and validation performance.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"d8c7e19988d4362ca9a7349afba11102","grade":false,"grade_id":"cell-d02a5030881356fc","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"all_tests_successful = True\nif not skip_training:\n    try:\n        # Test 1: Ensure training accuracy is within the correct range\n        max_tacc = max(results['metrics'][\"training_accuracy\"])\n        if not (0.4 <= max_tacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 1.3, Visible test: Training accuracy {max_tacc} is out of the expected range [0.4, 1].\")\n            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.4, 1].\")\n\n        # Test 2: Ensure accuracy is within the correct range\n        max_vacc = max(\n            results['metrics'][\"validation_accuracy\"])\n        if not (0.4 <= max_vacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 1.3, Visible test: Validation accuracy {max_vacc} is out of the expected range [0.4, 1].\")\n            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.4, 1].\")\n\n        if all_tests_successful:\n            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n\n    except AssertionError as e:\n        feedback_txt.append(f\"Task 1.3, Visible test failed: {e}\")\n        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n\nelse:\n    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e1c4c0eb2f067ee367f21f757efd4653","grade":false,"grade_id":"cell-0de54dbb185009a0","locked":true,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"72499014828444c8cb964d7828e74b4a","grade":true,"grade_id":"cell-307e993829722beb","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 2: Hyperparameter Tuning (5 Points)\n\nIn this task, you will enhance your model’s performance through targeted modifications to the architecture. The main objectives include optimizing performance on the training dataset. The goal is to implement various changes and observe how they affect the overall performance of the model in terms of training and validation results, the smoothness and stability of the training curves, the number of trainable parameters, and the training time.\n\n### Summary of Tasks for This Stage\n\n\n**Task 2.1: Increase Convolution Channels** (1 point)\n\n    Goal: Modify the model to increase the number of convolution channels to 128 across three layers.\n\n**Task 2.2: Add One Convolution Layer** (1 point)\n\n    Goal: Add an additional convolution layer to the model, creating four layers with 32 intermediate channels.\n\n**Task 2.3: Adjust Kernel Sizes (Smaller)** (1 point)\n\n    Goal: Modify the kernel sizes to be smaller than those used in the base model.\n\n**Task 2.4: Adjust Kernel Sizes (Larger)** (1 point)\n\n    Goal: Modify the kernel sizes to be larger than those used in the base model.\n\n**Task 2.5: Change Non-linearities (ReLU)** (1 point)\n\n    Goal: Replace the current activation functions with ReLU.\n\n### Deliverables from this task:\n\n* 'relu_model.pth'","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"eecae489fe93ca94bc3e295ac0e502da","grade":false,"grade_id":"cell-268197b5a15a08c7","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"markdown","source":"### Task 2.1. Increase Convolution Channels\n\nUse the base model from Task 1, increasing the convolutional channels from 32 to 128. Fill in the blanks in the cell below as instructed in the code.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"af4c2d9d3bacc157e059b15a991096a7","grade":false,"grade_id":"cell-1d13342c4b666297","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"increased_channel_model_config = copy.deepcopy(base_config)\n\nincreased_channel_model_config['conv_channels'] = [128, 128, 1]\n\nresults = train_and_test_your_model(increased_channel_model_config, '128_channel_model', False)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c18f89d02ec16e57d8e4ac9c56ba116a","grade":false,"grade_id":"cell-52f869716b6d4621","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(increased_channel_model_config['nb_basic_blocks'], increased_channel_model_config[\"conv_channels\"],\n                increased_channel_model_config[\"kernel_size\"], increased_channel_model_config[\"stride\"], \n                increased_channel_model_config[\"non_linearity\"], increased_channel_model_config[\"use_batchnorm\"], \n                increased_channel_model_config[\"use_dropout\"], increased_channel_model_config[\"apply_pooling\"]).to(device)\n\ndummy_input = torch.randn(1, 1, 22000).to(device)\ndummy_output = model(dummy_input)\n\n# Count the number of Conv1d layers and check their channels\nconv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\nconv1d_count = len(conv1d_layers)\n\n# Test the number of Conv1d layers\nif conv1d_count != 3:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.1, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n\nexpected_channels = [128, 128, 1]  # Expected output channels for the three layers\n\nfor i, layer in enumerate(conv1d_layers):\n    if layer.out_channels != expected_channels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 2.1, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        \n# Check the output shape\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.1, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n\n# Chech the number of trainable parameters\nnum_params = get_num_trainable_parameters(model)\nexpected_num_parameters = 183297\nif num_params != expected_num_parameters:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.1, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n\nif all_tests_successful: \n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d1ae42f0f2681cc1ffcbe58ac1e934e0","grade":true,"grade_id":"cell-8644dfbff88f3dac","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visible tests for checking the performance of the trained model\nall_tests_successful = True\nif not skip_training:\n    try:\n    \n        # Test 1: Ensure training accuracy is within the correct range\n        max_tacc = max(results['metrics'][\"training_accuracy\"])\n        if not (0.5 <= max_tacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 2.1, Visible test: Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n            \n        # Test 2: Ensure accuracy is within the correct range\n        max_vacc = max(results['metrics'][\"validation_accuracy\"])\n        if not (0.5 <= max_vacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 2.1, Visible test: Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n    \n        if all_tests_successful:\n            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n    \n    except AssertionError as e:\n        feedback_txt.append(f\"Task 2.1, Visible test: {e}\")\n        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n\nelse:\n    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"cd75611a90cb2208187da3dc0430dcbd","grade":true,"grade_id":"cell-1bea1284dfe26ccb","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.2: Add One Convolution Layer\n\nNext, use the base model from Task 1 (32 channels), adding one extra the convolutional layers to have 4 convolutional layers with 32 intermediate filters (channels). Fill in the blanks in the cell below as instructed in the code.\n\n**Hint**: Add one more layer in the `conv_channel` key for the `increased_basic_block_config`","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3ea91259f0f552d152af64543a8e2394","grade":false,"grade_id":"cell-d60b87270f811b88","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"increased_basic_block_config = copy.deepcopy(base_config)\n\n# Add one more convolutional layer with 32 intermediate channels\nincreased_basic_block_config['nb_basic_blocks'] = 4\nincreased_basic_block_config['conv_channels'] = [32, 32, 32, 1]\n\nresults = train_and_test_your_model(increased_basic_block_config, '4_basic_blocks', False)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"04bdccd7c364c482095b871d4ecddf3b","grade":false,"grade_id":"cell-cd4e236620bba643","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the cell below to verify the correctness of your solution for the model architecture.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"b2bb5cf63e7af54a7626385d228eb5fd","grade":false,"grade_id":"cell-79662b93b2f23fe4","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(increased_basic_block_config['nb_basic_blocks'], increased_basic_block_config[\"conv_channels\"],\n                increased_basic_block_config[\"kernel_size\"], increased_basic_block_config[\"stride\"], \n                increased_basic_block_config[\"non_linearity\"], increased_basic_block_config[\"use_batchnorm\"], \n                increased_basic_block_config[\"use_dropout\"], increased_basic_block_config[\"apply_pooling\"]).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\ndummy_output = model(dummy_input)\n\n# Count the number of Conv1d layers and check their channels\nconv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\nconv1d_count = len(conv1d_layers)\n\n# Test the number of Conv1d layers\nif conv1d_count != 4:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.2, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n\nexpected_channels = [32, 32, 32, 1]  # Expected output channels for the three layers\n\nfor i, layer in enumerate(conv1d_layers):\n    if layer.out_channels != expected_channels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 2.2, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        \n# Check the output shape\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.2, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n\n# Check the number of trainable parameters\nnum_params = get_num_trainable_parameters(model)\nexpected_num_parameters = 23329\nif num_params != expected_num_parameters:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.2, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    \nif all_tests_successful: \n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"bb420ad0d5c4ec17785d4e0b4d4fb798","grade":true,"grade_id":"cell-0d9169afb9854179","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.3: Adjust Kernel Sizes (Smaller)\nIn this task, you will modify the base model by using smaller kernel sizes for the convolutional layers. Set the kernel size to 7 and the stride to 3 and keeping the same convolutional channels as your initial implemetation. Fill in the blanks in the code cell below as directed to apply these changes.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c458f436942e25a11f998414ab673700","grade":false,"grade_id":"cell-f32c3e2b00a24dc6","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"reduced_kernel_size_and_stride_config = copy.deepcopy(base_config)\n\n#Use smaller kernel size and stride\nreduced_kernel_size_and_stride_config['kernel_size'] = 7\nreduced_kernel_size_and_stride_config['stride'] = 3\n\nresults = train_and_test_your_model(reduced_kernel_size_and_stride_config, 'reduced_kernel_size_and_stride', False)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c7bb774fb15da5bfbcfbaddabff6cec5","grade":false,"grade_id":"cell-3e864d3b29cc8373","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\n\nmodel = MyModel(reduced_kernel_size_and_stride_config['nb_basic_blocks'], reduced_kernel_size_and_stride_config[\"conv_channels\"], \n                reduced_kernel_size_and_stride_config['kernel_size'], reduced_kernel_size_and_stride_config['stride'], \n                reduced_kernel_size_and_stride_config['non_linearity'], reduced_kernel_size_and_stride_config['use_batchnorm'], \n                reduced_kernel_size_and_stride_config['use_dropout'], reduced_kernel_size_and_stride_config['apply_pooling']).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\ndummy_output = model(dummy_input)\n\n# Count the number of Conv1d layers and check their channels\nconv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\nconv1d_count = len(conv1d_layers)\n\n# Test the number of Conv1d layers\nif conv1d_count != 3:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.3, Expected 3 Conv1d layers, got {conv1d_count}.\")\n    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n\nexpected_channels = [32, 32, 1]  # Expected output channels for the three layers\nexpected_kernels = [7, 7, 7]\n\nfor i, layer in enumerate(conv1d_layers):\n    if layer.out_channels != expected_channels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 2.3, Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n    # Check the kernel size\n    if layer.kernel_size[0] != expected_kernels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 2.3, Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")    \n# Check the output shape\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.3, Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n\n# Chech the number of trainable parameters\nnum_params = get_num_trainable_parameters(model)\nexpected_num_parameters = 7681\nif num_params != expected_num_parameters:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.3, Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n\nif all_tests_successful: \n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"14d9c908934782e9f9d1d8ec65381992","grade":true,"grade_id":"cell-e268d234cf77f40f","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.4: Adjust Kernel Sizes (Larger)\nIn this step, return to the base model config and modify the kernel sizes of the convolutional layers to be larger. Use kernels of size 22 and a stride of 11. Complete the cell below by filling in the blanks as specified in the code.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"92fe5d97c30906afd248a85fe42c0d73","grade":false,"grade_id":"cell-11ffed8cc29c0f71","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"increased_kernel_size_and_stride_config = copy.deepcopy(base_config)\n\nincreased_kernel_size_and_stride_config['kernel_size'] = 22\nincreased_kernel_size_and_stride_config['stride'] = 11\n\nresults =  train_and_test_your_model(increased_kernel_size_and_stride_config, 'increased_kernel_size_and_stride', False)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"e7b547a9ba749593eb3f45a6755d8e83","grade":false,"grade_id":"cell-9cd7cb14774030ef","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the cell below to verify the correctness of your solution for the model architecture.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"1d33db8c15d3fbef801d32bab35be53d","grade":false,"grade_id":"cell-6d8ed9bec09c65d6","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(increased_kernel_size_and_stride_config[\"nb_basic_blocks\"], increased_kernel_size_and_stride_config[\"conv_channels\"], \n                increased_kernel_size_and_stride_config['kernel_size'], increased_kernel_size_and_stride_config['stride'], \n                increased_kernel_size_and_stride_config[\"non_linearity\"], increased_kernel_size_and_stride_config[\"use_batchnorm\"],\n                increased_kernel_size_and_stride_config[\"use_dropout\"], increased_kernel_size_and_stride_config[\"apply_pooling\"]).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\ndummy_output = model(dummy_input)\n\n# Count the number of Conv1d layers and check their channels\nconv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\nconv1d_count = len(conv1d_layers)\nexpected_kernels = [22, 22, 22]\n\n\n# Test the number of Conv1d layers\nif conv1d_count != 3:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.4, Expected 3 Conv1d layers, got {conv1d_count}.\")\n    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n\nexpected_channels = [32, 32, 1]  # Expected output channels for the three layers\n\nfor i, layer in enumerate(conv1d_layers):\n    if layer.out_channels != expected_channels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 2.4, Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n    # Check the kernel size\n    if layer.kernel_size[0] != expected_kernels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 2.4, Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")    \n\n# Check the output shape\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.4, Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n\n# Chech the number of trainable parameters\nnum_params = get_num_trainable_parameters(model)\nexpected_num_parameters = 24001\nif num_params != expected_num_parameters:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 2.4, Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n\nif all_tests_successful: \n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f6e9a9d8d0f4fcad1d26c9437bc8840c","grade":true,"grade_id":"cell-d789b5b8ee81acf9","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.5: Change Non-linearities\nFor this task, return to the base model and explore the impact of different non-linear activation functions on the model's performance.\n\nRe-create the base model by filling in the blanks in the cell below as instructed. Try using \"ReLU\" as an alternative activation function to see how that affect training and validation results.\n\nSave the model as **relu_model.pth** and submit it to Moodle along with your other files.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"50bd7bd4d2a93850d2e2c24bc3132cc1","grade":false,"grade_id":"cell-07ebd599973e351d","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"relu_model_config = copy.deepcopy(base_config)\n\nrelu_model_config['non_linearity'] = 'ReLU'\n\nresults = train_and_test_your_model(relu_model_config, 'relu_model', True)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9956e1a46da5681e25eb893daf02029c","grade":false,"grade_id":"cell-cd76eb212edfccfd","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the cell below to verify the correctness of your solution for the model architecture.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ec1f418fc38473398710e690b1a8d1a1","grade":false,"grade_id":"cell-c8c285cc82bdb4f6","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# Visible tests for checking the performance of the trained model\nall_tests_successful = True\nmodel = MyModel(relu_model_config['nb_basic_blocks'], relu_model_config[\"conv_channels\"], relu_model_config['kernel_size'], \n                relu_model_config['stride'], relu_model_config['non_linearity'], relu_model_config['use_batchnorm'],\n                relu_model_config['use_dropout'], relu_model_config['apply_pooling']).to(device)\nif not skip_training:\n    try:\n        # Test 0: Ensure non-linearity is ReLU\n        basic_blocks = [layer for layer in model.modules() if isinstance(layer, BasicBlock)]\n        for i, block in enumerate(basic_blocks):\n            if not isinstance(block.activation_fn, nn.ReLU):\n                all_tests_successful = False\n                feedback_txt.append(f\"Task 2.5, BasicBlock {i + 1} does not use ReLU as the activation function.\")\n                raise AssertionError(f\"BasicBlock {i + 1} does not use ReLU as the activation function.\")\n    \n        # Test 1: Ensure training accuracy is within the correct range\n        max_tacc = max(results['metrics']['training_accuracy'])\n        if not (0.5 <= max_tacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 2.5, Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n            \n        # Test 2: Ensure accuracy is within the correct range\n        max_vacc = max(results['metrics']['validation_accuracy'])\n        if not (0.5 <= max_vacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 2.5, Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n    \n        if all_tests_successful:\n            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n    \n    except AssertionError as e:\n        feedback_txt.append(f\"Task 2.5, {e}\")\n        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n\nelse:\n    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8ade90a68bacdbd2d8f3eb167b5467aa","grade":true,"grade_id":"cell-95e5355cd0d610b1","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5a3d82a35867a8430134ce4913c7dbd2","grade":true,"grade_id":"cell-ffbf6eae987ebc69","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 3: Optimization Practices (5 Points)\n\nIn this task, you will practice optimization techniques by experimenting with different optimizers, learning rates, and batch sizes. The main objectives include observing the performance and computational costs related to training time. Additionally, pay attention to the training curves: in deep learning, smoother training and validation curves are generally desirable, as they often indicate stable learning and consistent generalization.\n\nYou can achieve smoother training curves by adjusting the learning rate and increasing the batch size. Another important factor to consider is batch variability through data shuffling. While data shuffling can enhance training, very high learning rates, combined with shuffling, can negatively impact the smoothness and stability of the training process, potentially leading to unstable learning.\n\nBy completing this task, you will gain insights into how different optimization choices affect the training dynamics of your model.\n\n\n### Summary of Tasks for This Stage\n\n\n**Task 3.1: Experiment with Adam optimizer with different learning rates** (1 point)\n\n    Goal: Increase the learning rate in each test to find the optimal learning rate.\n\n**Task 3.2: Experiment with Adam optimizer with different learning rates and shuffling data in each batch** (1 point)\n\n    Goal: Allow the data to be shuffled in each batch, increase the learning rate, and observe its effect.\n\n**Task 3.3: Experiment with Adam optimizer and different batch sizes** (1 point)\n\n    Goal: Observe the effect of batch sizes on training stability and efficiency.\n\n**Task 3.4: Experiment with SGD optimizer with different learning rates and shuffling data in each batch** (1 point)\n\n    Goal: Observe the effect of data variability and learning rate in the SGD optimizer.\n\n**Task 3.5: Experiment with SGD optimizer and batch normalization layers** (1 point)\n\n    Goal: Observe the effect of batch normalization layers in the SGD optimizer.\n    \n\n### Deliverables from this task:\n\n* 'lr_model.pth'\n* 'shuffle_model.pth'\n* 'bs_model.pth'\n* 'SGD_model.pth'","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"cd7901c3d1f55cf5de303cf1a532f5f3","grade":false,"grade_id":"cell-60149db0f9f6712c","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"markdown","source":"### Task 3.1: Experiment with Adam optimizer with different learning rates\n\nUse the settings applied in Task 1 for the base model:\n\n- `batch_size = 8`\n- `optimizer = \"Adam\"`\n\nGradually increase the learning rate from 0.0001 (used in the base model) to observe its effect and find the optimal learning rate for this setting. In particular, test the following learning rates:\n\n- `lr = [0.0001, 0.001, 0.01, 0.1]`\n\nSave the best-performing model as **'lr_model.pth'** and submit it to Moodle along with your other files.\n\n**Goal**: Identify how different learning rates impact model performance, stability, and training efficiency for the current setup.\n\n","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"2bde008734387b479fe8f0bf8e52245b","grade":false,"grade_id":"cell-09f8bb5b3d20a5bc","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"optimizer_lr_config = copy.deepcopy(base_config)\n\nlearning_rates = [0.0001, 0.001, 0.01, 0.1]\nbest_result = None\nbest_lr = None\n\nfor lr in learning_rates:\n    config = copy.deepcopy(base_config)\n    config['optimizer_type'] = 'Adam'\n    config['learning_rate'] = lr\n    config['batch_size'] = 8\n\n    print(f\"\\nTraining with learning rate = {lr}\")\n    results = train_and_test_your_model(config, f'lr_{lr}', False)\n\n    # Suppose 'results' contains validation accuracy or loss; pick the best\n    val_acc = results['metrics'][\"validation_accuracy\"][-1]\n\n    if best_result is None or val_acc > best_result['metrics']['validation_accuracy'][-1]:\n        best_result = results\n        best_lr = lr\n\n# ✅ Retrain the best one and save it as lr_model.pth\nprint(f\"\\nBest learning rate: {best_lr}\")\nfinal_config = copy.deepcopy(base_config)\nfinal_config['optimizer_type'] = 'Adam'\nfinal_config['learning_rate'] = best_lr\nfinal_config['batch_size'] = 8\n\nresults = train_and_test_your_model(optimizer_lr_config, 'lr_model', True)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0920b85fb6b7dabfe05bfcd224fc56c5","grade":false,"grade_id":"cell-5545109c808f3795","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"73c09df6a8cb4f61a8de69edff925385","grade":true,"grade_id":"cell-4c63c220f514bf27","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 3.2: Experiment with Adam optimizer with different learning rates and shuffling data in each batch \n\nRepeat the experiments from Task 3.1, this time with data shuffling enabled to increase batch variability (by setting `shuffle=True`). Test the following learning rates:\n\n- `lr = [0.0001, 0.001, 0.01, 0.1]`\n\nSave the best-performing model as **'shuffle_model.pth'** and submit it to Moodle along with your other files.\n\n**Goal**: Identify how different learning rates impact model performance and stability in the presence of data variability.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"24bd87aa1010114153079a7976f8d068","grade":false,"grade_id":"cell-2a2eb72c7577ead5","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"shuffle_model_config = copy.deepcopy(base_config)\n\n#Use the Adam optimizer\nshuffle_model_config['optimizer_type'] = 'Adam'\n\n#Enable data shuffling\nshuffle_model_config['shuffle'] = True\n\n#Keep the same batch size as in Task 1\nshuffle_model_config['batch_size'] = 8\n\n#Try learning rates from Task 3.1\nlearning_rates = [0.0001, 0.001, 0.01, 0.1]\nbest_result = None\nbest_lr = None\n\nfor lr in learning_rates:\n    config = copy.deepcopy(shuffle_model_config)\n    config['learning_rate'] = lr\n\n    print(f\"\\nTraining with learning rate = {lr}, shuffle=True\")\n    results = train_and_test_your_model(config, f'shuffle_lr_{lr}', False)\n\n    # Select the best model based on validation accuracy\n    val_acc = results['metrics'][\"validation_accuracy\"][-1]\n\n    if best_result is None or val_acc > best_result['metrics']['validation_accuracy'][-1]:\n        best_result = results\n        best_lr = lr\n\n# ✅ Retrain using the best learning rate and save as shuffle_model.pth\nprint(f\"\\nBest learning rate with shuffle=True: {best_lr}\")\nfinal_config = copy.deepcopy(shuffle_model_config)\nfinal_config['learning_rate'] = best_lr\n\nresults = train_and_test_your_model(shuffle_model_config, 'shuffle_model', True)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"8b792ac868f4427b08e2251630e2496a","grade":false,"grade_id":"cell-e1b9ab40a7607bda","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"300c5d498c3790bedb67423ad32414b8","grade":true,"grade_id":"cell-0b2d57cb7c5290a5","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 3.3: Experiment with Adam optimizer and different batch sizes\n\nNow, set the optimizer to Adam with `lr = 0.001` and disable data shuffling (`shuffle=False`). Try different batch sizes as specified below:\n\n- `bs = [4, 8, 16, 32]`\n\nSave the best-performing model as **'bs_model.pth'** and submit it to Moodle along with your other files.\n\n**Optional:** You may repeat the experiments above with data shuffling enabled (`shuffle=True`) to observe the impact of batch size when data variability is increased.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"6f00f7f19ec0cdd262b1671df130aa09","grade":false,"grade_id":"cell-1cc1f418104f7a72","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"shuffle_model_config = copy.deepcopy(base_config)\n\nbs_model_config = copy.deepcopy(base_config)\n\n# Use Adam optimizer\nbs_model_config['optimizer_type'] = 'Adam'\n\n#Fix the learning rate\nbs_model_config['learning_rate'] = 0.001\n\n# Disable data shuffling\nbs_model_config['shuffle'] = False\n\n# Try multiple batch sizes\nbatch_sizes = [4, 8, 16, 32]\nbest_result = None\nbest_bs = None\n\nfor bs in batch_sizes:\n    config = copy.deepcopy(bs_model_config)\n    config['batch_size'] = bs\n\n    print(f\"\\nTraining with batch size = {bs}\")\n    results = train_and_test_your_model(config, f'bs_{bs}_model', False)\n\n    # Keep track of the best-performing configuration\n    val_acc = results['metrics'][\"validation_accuracy\"][-1]\n\n    if best_result is None or val_acc > best_result['metrics']['validation_accuracy'][-1]:\n        best_result = results\n        best_lr = lr\n\n# ✅ Retrain and save the best model\nprint(f\"\\nBest batch size: {best_bs}\")\nfinal_config = copy.deepcopy(bs_model_config)\nfinal_config['batch_size'] = best_bs\n\nresults = train_and_test_your_model(shuffle_model_config, 'bs_model', True)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a1d989f12dc6582e7bfba87e4cc62518","grade":false,"grade_id":"cell-70e75064e280599c","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0a635ec57c865c547e5f8774da428d31","grade":true,"grade_id":"cell-a2f1c3666e4ad7e7","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 3.4: Experiment with SGD optimizer with different learning rates and shuffling data in each batch\n\nRepeat the experiments from Task 3.2, keeping data shuffling enabled, and change the optimizer to SGD. Test the following learning rates:\n\n- `lr = [0.0001, 0.001, 0.01, 0.1]`\n\nSave the best-performing model as **'SGD_model.pth'** and submit it to Moodle along with your other files.\n\n**Goal**: Observe the different behaviors of Adam and SGD optimizers, and examine how the choice of optimizer impacts the selection and effect of learning rate on model performance.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"e77b52914e92ec19595004e261068180","grade":false,"grade_id":"cell-d31d06e19f5dd50e","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"sgd_model_config = copy.deepcopy(shuffle_model_config)\n\n# ✅ Use SGD optimizer\nsgd_model_config['optimizer_type'] = 'SGD'\n\n# ✅ Ensure data shuffling is enabled\nsgd_model_config['shuffle'] = True\n\n# ✅ Try multiple learning rates\nlearning_rates = [0.0001, 0.001, 0.01, 0.1]\nbest_result = None\nbest_lr = None\n\nfor lr in learning_rates:\n    config = copy.deepcopy(sgd_model_config)\n    config['learning_rate'] = lr\n\n    print(f\"\\nTraining with SGD optimizer and learning rate = {lr}\")\n    results = train_and_test_your_model(config, f'SGD_lr_{lr}_model', False)\n\n    # Track the best-performing configuration\n    val_acc = results['metrics'][\"validation_accuracy\"][-1]\n\n    if best_result is None or val_acc > best_result['metrics']['validation_accuracy'][-1]:\n        best_result = results\n        best_lr = lr\n\n# ✅ Retrain and save the best-performing model\nprint(f\"\\nBest learning rate for SGD: {best_lr}\")\nfinal_config = copy.deepcopy(sgd_model_config)\nfinal_config['learning_rate'] = best_lr\n\nresults = train_and_test_your_model(sgd_model_config, 'SGD_model', True)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"4205781c5382821b0fc61c92a6bcbddc","grade":false,"grade_id":"cell-b9aeb28c3d136e59","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9a4a4e4975d7f26748af34b249cd246b","grade":true,"grade_id":"cell-04784ef159da0b85","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 3.5: Experiment with SGD optimizer and normalization layers\n\nNext, test the effect of batch normalization layers on the SGD optimizer using the following setup for training:\n\n- Batch data shuffling disabled (`shuffle=False`)\n- `learning_rate = 0.01`\n- `batch_size = 16`\n- `optimizer_type = SGD`\n\nUse the base model, then add a batch normalization layer within each convolutional block. Place this layer directly after each convolution layer and before applying the non-linearity function, as indicated in the code.\n\n`Hint: set use_batchnorm to true`\n\n**useful link**: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ccbccc137576be5ce84a47cb07400d87","grade":false,"grade_id":"cell-c09d3ce8367c660b","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"normalization_model_config = copy.deepcopy(base_config)\n\nnormalization_model_config[\"shuffle\"] = False\nnormalization_model_config[\"optimizer_type\"] = \"SGD\"\nnormalization_model_config[\"batch_size\"] = 16\nnormalization_model_config[\"learning_rate\"] = 0.01\nnormalization_model_config[\"use_batchnorm\"] = True\n\nresults = train_and_test_your_model(normalization_model_config, 'normalization_model', False)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"249775001dcff84a36b666fd9ff4a1a5","grade":false,"grade_id":"cell-2d839e2965a855e3","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(normalization_model_config['nb_basic_blocks'], normalization_model_config[\"conv_channels\"], normalization_model_config['kernel_size'], \n                normalization_model_config['stride'], normalization_model_config['non_linearity'], normalization_model_config['use_batchnorm'], \n                normalization_model_config['use_dropout'], normalization_model_config['apply_pooling']).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\n\n# Dictionary to hold the execution order of each BasicBlock's layers\nlayer_execution_order = {}\n# Function to capture forward pass order of layers within each BasicBlock\ndef track_execution_order(module, input, output, name):\n    layer_types = []\n    for sub_module in module.children():  # Iterate through layers within BasicBlock\n        layer_types.append(type(sub_module))\n    layer_execution_order[name] = layer_types\n\n# Register hooks on each BasicBlock to capture layer order in forward pass\nfor name, module in model.named_modules():\n    if isinstance(module, BasicBlock):\n        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n\n# Run the model forward pass to trigger hooks\ndummy_output = model(dummy_input)\n\n# Define the expected order of layer types for BasicBlock\nexpected_order = [nn.Conv1d, nn.BatchNorm1d, nn.Tanh]  \n\n# Check if each BasicBlock followed the expected order\nfor name, order in layer_execution_order.items():\n    # Modify expected_order based on the chosen activation in model\n    current_expected_order = expected_order[:]\n    activation_fn_type = type(model.layers[0].activation_fn)  # Get the actual activation type\n    current_expected_order[-1] = activation_fn_type\n    \n    if order != current_expected_order:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 3.5, Visible test: {name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]}.\"\n                           f\"but got {[cls.__name__ for cls in order]}.\")\n        raise AssertionError(\n            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n            f\"but got {[cls.__name__ for cls in order]}.\"\n        )\n\n# Check output shape and range for LogSoftmax\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 3.5, Visible test: Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n\n# Final success message if all tests pass\nif all_tests_successful:\n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"05713f37ccda1138648dada4c62ce7d5","grade":true,"grade_id":"cell-a936094e8faa2af5","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 4: Regularization Practices (5 Points)\n\nIn Task 2 you have tried differnt hyperparameter tuning techniques to increase the performance of the model on the training set. In this task, you will practice regularization techniques to help the model to generalize to unseen data, i.e., to increase the performance on the validation set. You are asked to start with a model that achieves a good performance on the training set compared to the base model. However, the number of trainable parameters in this model is large which leads to over-fitting as observed in the training and validation curves. Here we try different techniques to decreadse the number of trainable påarameters and to increase the model performance on validation set. \n\n\n### Summary of Tasks for This Stage\n\n\n**Task 4.1: Experiment with normalization layers** (1 point)\n\n    Goal: Observe the effect of batch normalization in model generalization and training stability.\n\n**Task 4.2: Experiment with dropout layers** (1 point)\n\n    Goal: Observe the effect of dropout layers in model generalization.\n\n**Task 4.3: Experiment with efficient model architecture through pooling layers** (3 point)\n\n    Goal: Observe the effect of efficient model design through adjusting receptive field of layers.\n\n\n### Deliverables from this task:\n\n* 'normalized_model.pth'\n* 'pooled_model.pth'","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9c0f2b745b9cc0c4929ec6ce7b0d0d84","grade":false,"grade_id":"cell-32584ff422efb7fb","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"markdown","source":"### Model Architecture: \n\nFill in the blanks as instructed in the code to design the model architecture similar to the base config model but with four convolutional blocks, where the number of filters (channels) in each convolutional layer is set to 128.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0817b2b9e75055ed25c9adb268440201","grade":false,"grade_id":"cell-11383a0618897151","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"channel_128_config = copy.deepcopy(base_config)\n\nchannel_128_config['conv_channels'] = [128, 128, 128, 1]\nchannel_128_config['nb_basic_blocks']= 4\n\nresults = train_and_test_your_model(channel_128_config, 'channel_128', False)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"17ddd288a971b280d79b51cf257932b9","grade":false,"grade_id":"cell-0b347d2067197eab","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the cell below to verify the correctness of your model architecture solution.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a7e258d88c9bab0b3f13d219de575199","grade":false,"grade_id":"cell-16a40bfe843509d1","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(channel_128_config['nb_basic_blocks'], channel_128_config['conv_channels'], channel_128_config['kernel_size'],\n                channel_128_config['stride'], channel_128_config['non_linearity'],\n                channel_128_config['use_batchnorm'], channel_128_config['use_dropout'], channel_128_config['apply_pooling']).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\ndummy_output = model(dummy_input)\n\n# Count the number of Conv1d layers and check their channels\nconv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\nconv1d_count = len(conv1d_layers)\n\n# Test the number of Conv1d layers\nif conv1d_count != 4:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n\nexpected_channels = [128, 128, 128, 1]  # Expected output channels for the three layers\n\nfor i, layer in enumerate(conv1d_layers):\n    if layer.out_channels != expected_channels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 4, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        \n# Check the output shape\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n\n# Chech the number of trainable parameters\nnum_params = get_num_trainable_parameters(model)\nexpected_num_parameters = 363649\nif num_params != expected_num_parameters:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n\nif all_tests_successful: \n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"4e3f03b96a407f547e86b88817806aa1","grade":true,"grade_id":"cell-dc18644669c4f119","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 4.1: Experiment with normalization layers\n\nAdd a batch normalization layer after the convolution layer and before applying the non-linearity function. Train the model, monitor the behavior of the training and validation curves, and observe how the normalization layer affects the validation performance.\n\nSave the model as **'normalized_model.pth'** and submit it to Moodle along with your other files.\n","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"cc3bd5a0db96b93eccd360754256e15e","grade":false,"grade_id":"cell-4c08af9dfcaefb0c","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"normalized_config = copy.deepcopy(base_config)\n\nnormalized_config['use_batchnorm'] = True\n\nresults = train_and_test_your_model(normalized_config, 'normalized_model', True)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6441941587e9ba73972b95932f431c98","grade":false,"grade_id":"cell-33eff9feef979ca1","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the cell below to verify the correctness of your model architecture solution.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"b17d0e1bcd15b7b40f8a77a459105d6a","grade":false,"grade_id":"cell-82a0dbe4e98847d8","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(normalized_config['nb_basic_blocks'], normalized_config['conv_channels'], normalized_config['kernel_size'],\n                normalized_config['stride'], \"ReLU\",\n                normalized_config['use_batchnorm'], normalized_config['use_dropout'], normalized_config['apply_pooling']).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\n\n# Dictionary to hold the execution order of each BasicBlock's layers\nlayer_execution_order = {}\n# Function to capture forward pass order of layers within each BasicBlock\ndef track_execution_order(module, input, output, name):\n    layer_types = []\n    for sub_module in module.children():  # Iterate through layers within BasicBlock\n        layer_types.append(type(sub_module))\n    layer_execution_order[name] = layer_types\n\n# Register hooks on each BasicBlock to capture layer order in forward pass\nfor name, module in model.named_modules():\n    if isinstance(module, BasicBlock):\n        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n\n# Run the model forward pass to trigger hooks\ndummy_output = model(dummy_input)\n\n# Define the expected order of layer types for BasicBlock\nexpected_order = [nn.Conv1d, nn.BatchNorm1d, nn.ReLU]  \n\n# Check if each BasicBlock followed the expected order\nfor name, order in layer_execution_order.items():\n    # Modify expected_order based on the chosen activation in model\n    current_expected_order = expected_order[:]\n    activation_fn_type = type(model.layers[0].activation_fn)  # Get the actual activation type\n    current_expected_order[-1] = activation_fn_type\n    \n    if order != current_expected_order:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 4.1, Visible test: {name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n                            f\"but got {[cls.__name__ for cls in order]}.\")\n        raise AssertionError(\n            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n            f\"but got {[cls.__name__ for cls in order]}.\"\n        )\n\n# Check output shape and range for LogSoftmax\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4.1, Visible test: Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n\n# Final success message if all tests pass\nif all_tests_successful:\n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"da198b3b19fd38edfcdf22ad98877909","grade":true,"grade_id":"cell-93c7f2a7fe78fad4","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8c144649060206ec1f284fa082c2813b","grade":true,"grade_id":"cell-2cc5ef3d576fb0ec","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 4.2: Experiment with dropout layers\n\nKeep the normalization layers and add a dropout layer as the last layer of the convolutional block. \n\nTrain the model, monitor the behavior of the training and validation curves, and observe how the dropout layers affects the validation performance.\n\n`Hint:` set dropout to True in your config","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"a5fd7d7801be7277214bf7722dce25cd","grade":false,"grade_id":"cell-38d80197be5fe59b","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"dropout_config = copy.deepcopy(base_config)\n\ndropout_config['use_batchnorm'] = True\ndropout_config['use_dropout'] = True\n\nresults = train_and_test_your_model(dropout_config, 'dropout_model', False)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"40282513a5429f53af89a30698110928","grade":false,"grade_id":"cell-1819914b7ffa862a","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the cell below to verify the correctness of your solution for the model architecture.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ee6d0484d0e47000d17ccf1b0c4ad824","grade":false,"grade_id":"cell-ad683def87f23b99","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(dropout_config['nb_basic_blocks'], dropout_config['conv_channels'], dropout_config['kernel_size'],\n                dropout_config['stride'], \"ReLU\",\n                dropout_config['use_batchnorm'], dropout_config['use_dropout'], dropout_config['apply_pooling']).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\n\n# Dictionary to hold the execution order of each BasicBlock's layers\nlayer_execution_order = {}\n\n# Function to capture forward pass order of layers within each BasicBlock\ndef track_execution_order(module, input, output, name):\n    layer_types = []\n    for sub_module in module.children():  # Iterate through layers within BasicBlock\n        layer_types.append(type(sub_module))\n    layer_execution_order[name] = layer_types\n\n# Register hooks on each BasicBlock to capture layer order in forward pass\nfor name, module in model.named_modules():\n    if isinstance(module, BasicBlock):\n        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n\n# Run the model forward pass to trigger hooks\ndummy_output = model(dummy_input)\n\n# Define the expected order of layer types for BasicBlock with Dropout\nexpected_order = [nn.Conv1d, nn.BatchNorm1d, nn.ReLU, nn.Dropout]  \n\n# Check if each BasicBlock followed the expected order\nfor name, order in layer_execution_order.items():\n    # Modify expected_order based on the chosen activation in model\n    current_expected_order = expected_order[:]\n    activation_fn_type = type(model.layers[0].activation_fn)  # Get the actual activation type\n    current_expected_order[2] = activation_fn_type  # Ensure activation function is dynamically set\n\n    if order != current_expected_order:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 4.2, Visible test: {name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n                            f\"but got {[cls.__name__ for cls in order]}.\")\n        raise AssertionError(\n            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n            f\"but got {[cls.__name__ for cls in order]}.\"\n        )\n\n# Check output shape and range for LogSoftmax\nexpected_shape = (1, 1)\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4.2, Visible test: Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n\n# Final success message if all tests pass\nif all_tests_successful:\n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"50af691c314a4d3d5253cfc373c319fe","grade":true,"grade_id":"cell-e862c8ea453d2fbd","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 4.3: Experiment with efficient model architecture through pooling layers\n\nAs the final step, try to increase the model efficiency through a wiser design of the model architecture. In a deep learning model, while using a stack of convolutional blocks, it is common practice to use pooling layers between convolutional layers to decrease the dimension of the data and the resolution of deeper layers. This helps make the model lighter by reducing the number of trainable parameters and, at the same time, increases the model's performance by helping it focus on different feature types at different layers. For example, in the case of audio processing, the shallower layers can focus on finding variations in short time windows, while deeper layers can focus on detecting longer variations.\n\nIn this task, you are asked to follow the same logic and modify the model architecture using pooling layers.\n\nSet the configuration for you config according to following:\n\n- kernel_size set to 10\n- stride set to 2\n- non_linearity to ReLU\n- normalization, dropout and pooling set to True\n- The conv_channel will be of size 3 with the following configuration:\n  - first layer: input channel: 1, output channel: 8\n  - second layer: input channel: 8, output channel: 16\n  - third layer: input channel: 16, output channel: 32\n\n    `Hint`: set the conv_channel to 8,16,32\n\nSave the trained model as **'pooled_model.pth'** and submit it to Moodle along with your other files.","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c8a4e25f6e1fa43f2dee5aa23f179dfb","grade":false,"grade_id":"cell-fd1f1415b1ac5fa0","locked":true,"schema_version":3,"solution":false,"task":false}}},{"cell_type":"code","source":"pooling_config = copy.deepcopy(base_config)\n\npooling_config['kernel_size'] = 10\npooling_config['stride'] = 2\npooling_config['non_linearity'] = \"ReLU\"  # use ReLU activation\npooling_config['use_batchnorm'] = True     # include normalization\npooling_config['use_dropout'] = True           # include dropout\npooling_config['apply_pooling'] = True           # include pooling layers\n\n# Set the conv channels: input and output channels for each layer\npooling_config['conv_channels'] = [8, 16, 32]\n\nresults = train_and_test_your_model(pooling_config, 'pooled_model', True)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2e6e879f84c88cfeac2bcc203cbba29d","grade":false,"grade_id":"cell-42da8962c7e599e0","locked":false,"schema_version":3,"solution":true,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visible tests here\nall_tests_successful = True\nmodel = MyModel(pooling_config['nb_basic_blocks'], pooling_config['conv_channels'], pooling_config['kernel_size'], \n                pooling_config['stride'], pooling_config['non_linearity'], pooling_config['use_batchnorm'], \n                pooling_config['use_dropout'], pooling_config['apply_pooling']).to(device)\ndummy_input = torch.randn(1, 1, 22000).to(device)\ndummy_output = model(dummy_input)\n\n# Count the number of Conv1d layers and check their channels\nconv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\nconv1d_count = len(conv1d_layers)\n\n# Test the number of Conv1d layers\nexpected_conv_count = 3  # Number of Conv1d layers expected\nif conv1d_count != expected_conv_count:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4.3, Visible test: Expected {expected_conv_count} Conv1d layers, got {conv1d_count}.\")\n    raise AssertionError(f\"Expected {expected_conv_count} Conv1d layers, got {conv1d_count}.\")\n\n# Check expected output channels\nexpected_channels = [8, 16, 32]  # Expected output channels for the three Conv1d layers\n\nfor i, layer in enumerate(conv1d_layers):\n    if layer.out_channels != expected_channels[i]:\n        all_tests_successful = False\n        feedback_txt.append(f\"Task 4.3, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n\n# Check the output shape\nexpected_shape = (1, 1)  # Expected output shape\nif dummy_output.shape != expected_shape:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4.3, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n\n# Check the number of trainable parameters\ndef get_num_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nnum_params = get_num_trainable_parameters(model)\nexpected_num_parameters = 6681  # Expected number of trainable parameters\nif num_params != expected_num_parameters:\n    all_tests_successful = False\n    feedback_txt.append(f\"Task 4.3, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n\nif all_tests_successful: \n    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n    print(f\"\\033[92m{success_str}\\033[0m\")\n","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"dbda612464e8102c2a945de83d26b118","grade":true,"grade_id":"cell-3c72ba37ea31a699","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visible tests for checking the performance of the trained model\nall_tests_successful = True\nif not skip_training:\n    try:  \n        # Test 1: Ensure training accuracy is within the correct range\n        max_tacc = max(results['metrics'][\"training_accuracy\"])\n        if not (0.5 <= max_tacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 4.3, Visible test: Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n            \n        # Test 2: Ensure accuracy is within the correct range\n        max_vacc = max(results['metrics'][\"training_accuracy\"])\n        if not (0.5 <= max_vacc <= 1):\n            all_tests_successful = False\n            feedback_txt.append(f\"Task 4.3, Visible test: Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n    \n        if all_tests_successful:\n            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n    \n    except AssertionError as e:\n        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n\nelse:\n    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b04e1ce2a35d5e3c67c751f5dcb09105","grade":true,"grade_id":"cell-8668903249ced1e8","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1b564d08bfe96e324f01ca6087ecf50f","grade":true,"grade_id":"cell-14856483d61954ab","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do not delete this cell","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"cfb948a55c0e0ec7eb16b9eb26e51425","grade":true,"grade_id":"cell-9229ad2e11cd546c","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"trusted":true,"execution":{"execution_failed":"2025-11-10T20:21:27.440Z"}},"outputs":[],"execution_count":null}]}