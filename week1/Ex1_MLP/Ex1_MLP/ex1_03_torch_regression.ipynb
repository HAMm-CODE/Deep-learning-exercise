{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d51566d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6d51566d",
        "tags": []
      },
      "source": [
        "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution. Please read all the instructions in each notebook carefully.**  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a31c0f3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2a31c0f3",
        "tags": []
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "## Part 3. Implement and train a multilayer perceptron (MLP) using PyTorch\n",
        "\n",
        "In the final part of this assignment, you will implement and train a **Multilayer Perceptron (MLP)** for a regression problem using PyTorch.\n",
        "\n",
        "### Objective\n",
        "\n",
        "This assignment aims to help you understand the basic concepts and operations involved in building and training neural networks on PyTorch.\n",
        "\n",
        "You will follow the steps below:\n",
        "1. Data Generation: Construction of synthetic dataset for training. No implementation is required for this step, the function is provided.\n",
        "2. MLP Model Construction: Defining MLP architecture with given parameters and implementing forward pass in PyTorch.\n",
        "3. Training the Network: Implementating the training loop including loss calculation, backpropagation, and parameter updates using PyTorch's built-in methods.\n",
        "\n",
        "If you are new to PyTorch, you can review the following [introductory materials](https://pytorch.org/tutorials/beginner/basics/intro.html) to get familiar with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f785ee6f",
      "metadata": {
        "deletable": false,
        "deleteable": false,
        "editable": false,
        "id": "f785ee6f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "764c712370510d109ba89671f84a8abd",
          "grade": true,
          "grade_id": "cell-2eb5f9d97ca26051",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8215eef-3ca7-4cac-a902-931832c52e43",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f8215eef-3ca7-4cac-a902-931832c52e43",
        "tags": []
      },
      "source": [
        "### Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e228f8f0-73e8-429b-b7b6-f7aaf609fad9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e228f8f0-73e8-429b-b7b6-f7aaf609fad9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec085da",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bec085da",
        "tags": []
      },
      "source": [
        "### 1. Data Generation\n",
        "\n",
        "In this task, we will work with **2D data** where each data point has two features. While this is a basic example, real-world deep learning applications often deal with higher-dimensional data.\n",
        "\n",
        "We will generate synthetic data based on $y = x_1^2 +  x_2^2 + \\text{noise}$. The added `noise` is used to simulate randomness in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f528d336",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "deletable": false,
        "editable": false,
        "id": "f528d336",
        "outputId": "d95f77df-a1ad-4066-f19a-ed8ce8e44b0c",
        "tags": []
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def generate_data_2d(num_samples=1000):\n",
        "    # Generate random input data with shape (num_samples, 2)\n",
        "    np.random.seed(4)\n",
        "    x = np.random.randn(num_samples, 2)\n",
        "    noise = np.random.randn(num_samples, 1) * 0.1  # Add small noise\n",
        "\n",
        "    # Compute the output with the defined pattern\n",
        "    y = np.sum(x**2, axis=1, keepdims=True) + noise\n",
        "\n",
        "    x = torch.FloatTensor(x)\n",
        "    y = torch.FloatTensor(y)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def display_data(x,y):\n",
        "    fig = plt.figure(figsize=(6,6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(x[:, 0].numpy(), x[:, 1].numpy(), y[:,0].numpy(), c='b', marker='o')\n",
        "\n",
        "    ax.set_xlabel('Feature 1 (x1)')\n",
        "    ax.set_ylabel('Feature 2 (x2)')\n",
        "    ax.set_zlabel('Target (y)', labelpad=0)\n",
        "    ax.set_title('3D Plot of Synthetic Data with Two Features')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "x, y = generate_data_2d(1000)\n",
        "display_data(x,y)\n",
        "\n",
        "print(f'Shape of the input  x: {x.numpy().shape}')\n",
        "print(f'Shape of the target y: {y.numpy().shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62bfc9e0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "62bfc9e0",
        "tags": []
      },
      "source": [
        "### 2. MLP Model for Regression\n",
        "\n",
        "In this step, you will define a Multilayer Perceptron (MLP) model. You are provided with a template for the model class. Observe that the model inherits from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), which is the base class for all the neural networks in PyTorch. The backward pass is handled automatically by PyTorch. Commonly, there is no need to define the backward pass unless you need to customize the gradient flow.\n",
        "\n",
        "##### Steps to follow:\n",
        "1. Initialize the layers: You will define the layers using [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) and [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html). The model architecture should have the following structure:\n",
        "    -  Input layer: Takes 2 features.\n",
        "    -  Three hidden layers with 12, 10, and 6 units, respectively. Each layer is followed by a ReLU activation.\n",
        "    -  Output layer: Produces a single output.\n",
        "2. Forward pass: Implement the `forward()` method to define the data flow in the model.\n",
        "\n",
        "\n",
        "\n",
        "##### Hints:\n",
        "- You can simplify the model definition by using [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). It allows you to group the layers and activation into a sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009ff2f0",
      "metadata": {
        "deletable": false,
        "editable": true,
        "id": "009ff2f0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c9868892af7d8442cec17e15923beb73",
          "grade": false,
          "grade_id": "cell-3ffc7811d938ccb1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features=2, hidden_size1=12, hidden_size2=10, hidden_size3=6, out_features=1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.hidden_size3 = hidden_size3\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size2, hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size3, out_features)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0198e117-a60a-4b00-83a7-9f94ed4b2b0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "0198e117-a60a-4b00-83a7-9f94ed4b2b0b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2c056c28fb5a4123d04e5f203e0d2ec9",
          "grade": true,
          "grade_id": "cell-9769611741d62d75",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "899bd704-60f3-4c44-8a5b-66a443bcea2d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "feedback_txt = []\n",
        "\n",
        "# This cell checks the number of layers\n",
        "def test_layers():\n",
        "\n",
        "    all_tests_successful = True\n",
        "    model = MLP()\n",
        "    relu_count, linear_count = 0, 0\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.ReLU):\n",
        "            relu_count += 1\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            linear_count += 1\n",
        "\n",
        "    if relu_count == 0:\n",
        "        all_tests_successful = False\n",
        "        msg = \"At least one ReLU layer is expected, but got 0.\"\n",
        "        feedback_txt.append(f\"Visible test: {msg}\")\n",
        "        raise AssertionError(msg)\n",
        "\n",
        "    if linear_count != 4:\n",
        "        all_tests_successful = False\n",
        "        msg = f\"Expected 4 fully connected (Linear) layers, but got {linear_count}.\"\n",
        "        feedback_txt.append(f\"Visible test: {msg}\")\n",
        "        raise AssertionError(msg)\n",
        "\n",
        "    if all_tests_successful:\n",
        "        print(\"\\033[92mVisible test for layer count passed successfully!\\033[0m\")\n",
        "\n",
        "test_layers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56dd41b5-f5a5-47e2-b9e5-05357caf1f4e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "56dd41b5-f5a5-47e2-b9e5-05357caf1f4e",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8fea673af75619e3ac03dde0dac49aeb",
          "grade": true,
          "grade_id": "cell-9001bddef2ceecd2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This cell contains hidden test cases that will be evaluated after submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "078b7ba9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "078b7ba9",
        "tags": []
      },
      "source": [
        "### 3. Train the MLP network\n",
        "\n",
        "In this section, you will implement the training loop for the Multilayer Perceptron.\n",
        "\n",
        "Before diving into the training loop, let us define helper functions that we will use to simplify the workflow and visualize the training mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7462421d-cc4c-49ea-af8a-83290eb991d8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7462421d-cc4c-49ea-af8a-83290eb991d8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def init_model(learning_rate=0.05):\n",
        "    \"\"\"\n",
        "    Initializes the model, loss function, and optimizer.\n",
        "\n",
        "    Args:\n",
        "    - learning_rate (float): Learning rate.\n",
        "\n",
        "    Returns:\n",
        "    - model (MLP): An instance of the MLP model.\n",
        "    - criterion (nn.MSELoss): Mean squared error loss function for regression.\n",
        "    - optimizer (torch.optim.Adam): Adam optimizer for updating model weights.\n",
        "\n",
        "    Usage:\n",
        "    Call this function before the training starts.\n",
        "    \"\"\"\n",
        "    model = MLP()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    return model, criterion, optimizer\n",
        "\n",
        "# Function to visualize the loss curve after training\n",
        "def plot_loss_curve(losses, epochs):\n",
        "    \"\"\"\n",
        "    Plots the training loss over epochs after the training loop.\n",
        "\n",
        "    Args:\n",
        "    - losses (list): List of loss values for each epoch.\n",
        "    - epochs (int): Total number of epochs.\n",
        "\n",
        "    Usage:\n",
        "    After the training is completed, call this function to visualize how the training loss has changed over time.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(1, epochs + 1), losses, 'b', label='Training Loss')\n",
        "    ax.grid(True)\n",
        "    ax.set_title('Training Loss Curve')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ffcab7-cd87-4590-a811-c58c8be2e9f2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "54ffcab7-cd87-4590-a811-c58c8be2e9f2",
        "tags": []
      },
      "source": [
        "The model, loss function, and optimizer are instantiated (`init_model`) outside of the training loop. You will now implement the training loop.\n",
        "\n",
        "##### Steps to follow for each iteration (epoch):\n",
        "\n",
        "1. Set all gradients to zero: Use optimizer's `zero_grad()` method  to prevent gradient accumulation from previous iterations.\n",
        "2. Perform the forward pass: Pass the input through the model (you do not have to explicitly call the `forward()` method as PyTorch already handles this.)\n",
        "3. Compute the loss (see [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)).\n",
        "4. Backpropagate **the loss** (see [backward](https://pytorch.org/docs/stable/generated/torch.autograd.backward.html)).\n",
        "5. Update the parameters: Use the optimizer's `step()` method.\n",
        "6. Store the **numerical** loss values after each iteration to visualize the loss curve at the end of the training.\n",
        "  \n",
        "##### Recommended parameters:\n",
        "1. You **must** use Mean Squared Error (MSE) as the loss function (see [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)) and the Adam optimizer (see [optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)).\n",
        "2. A recommended learning rate is 0.05.\n",
        "3. Set the number of epochs to at least 100 for the model to converge.\n",
        "\n",
        "\n",
        "##### Important notes:\n",
        "- For simplicity, use all the training data to compute the gradients in each iteration (this is called full-batch training).\n",
        "- Try running your model without calling zero_grad() and **observe the effect of gradient accumulation** on the loss curve.\n",
        "- When storing the loss values ensure that you save the numerical value, not the tensor itself.\n",
        "- You can experiment with the model parameters (such as learning rate and the number of epochs) but you must use the provided x and y values as the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93da9a08",
      "metadata": {
        "deletable": false,
        "editable": true,
        "id": "93da9a08",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1f66e1432459dbca12f22b237cfeae7f",
          "grade": false,
          "grade_id": "cell-8ce61a1fbb5c4cd8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_mlp(model, criterion, optimizer, x, y, epochs=200):\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return losses  # Return to visualize the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9adc4b45-d85a-4257-be4c-887c93184629",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "9adc4b45-d85a-4257-be4c-887c93184629",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d5f9e433ef28bb0f85e5146416936080",
          "grade": true,
          "grade_id": "cell-8ced7efce4a1e4eb",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "d72d2fc4-10cd-4836-9e4c-8d0047b1174e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This cell tests the training code\n",
        "from unittest.mock import patch\n",
        "def test_train_calls():\n",
        "    all_tests_successful = True\n",
        "    model, criterion, optimizer = init_model(0.05)\n",
        "    with patch('torch.Tensor.backward') as mock_backward, patch.object(optimizer, 'step') as mock_step:\n",
        "        train_mlp(model, criterion, optimizer, x, y, epochs=10)\n",
        "        if not mock_backward.called:\n",
        "            all_tests_successful = False\n",
        "            msg = \"You forgot to calculate the gradients.\"\n",
        "            feedback_txt.append(f\"Visible test: {msg}\")\n",
        "            raise AssertionError(msg)\n",
        "        if not mock_step.called:\n",
        "            all_tests_successful = False\n",
        "            msg = \"Visible test: You forgot to update the weights.\"\n",
        "            feedback_txt.append(f\"Visible test: {msg}\")\n",
        "            raise AssertionError(msg)\n",
        "\n",
        "        if all_tests_successful:\n",
        "            print(\"\\033[92mVisible test passed.\\033[0m\")\n",
        "test_train_calls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8591a8-706b-4685-b7f0-743b63b41062",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6c8591a8-706b-4685-b7f0-743b63b41062",
        "tags": []
      },
      "outputs": [],
      "source": [
        "x, y = generate_data_2d(1000) # DO NOT OVERWRITE THIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccafaf3b-aca4-4dde-8695-b0fbb4e075bb",
      "metadata": {
        "deletable": false,
        "editable": true,
        "id": "ccafaf3b-aca4-4dde-8695-b0fbb4e075bb",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "979d180869db59bc1540d64a030d82ae",
          "grade": false,
          "grade_id": "cell-d41f6738afd2d261",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Using recommended parameters to train the model (you can modify these as desired)\n",
        "# You are not expected to implement anything\n",
        "# To continue using the default parameters, remove raise NotImplementedError()\n",
        "num_epochs = 200\n",
        "learning_rate = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b8e2a2-d8f9-4c0f-85f3-db5e3e6ed170",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "deletable": false,
        "editable": false,
        "id": "47b8e2a2-d8f9-4c0f-85f3-db5e3e6ed170",
        "outputId": "2a897072-c2d1-4128-f130-191f8b8d4742",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize the model, criterion, and optimizer\n",
        "model, criterion, optimizer = init_model(learning_rate)\n",
        "losses = train_mlp(model, criterion, optimizer, x, y, epochs=num_epochs)\n",
        "plot_loss_curve(losses, len(losses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357ed325",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "357ed325",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "68fa8889f7169901206c256d77bc8084",
          "grade": true,
          "grade_id": "cell-9a0d1f29b57beb90",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test cell\n",
        "# Ensure your model achieves a low loss and the plot matches expected outputs.\n",
        "final_loss = losses[-1]\n",
        "try:\n",
        "    assert final_loss < 0.25, f'Loss is too high, got {final_loss}, check your implementation.'\n",
        "except AssertionError as e:\n",
        "    feedback_txt.append(f'Visible test, Loss is too high, got {final_loss}, check your implementation.')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ab161a-00ff-4a60-8f2a-b1abdcbff98d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e7ab161a-00ff-4a60-8f2a-b1abdcbff98d",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3a06cc58ba35a528c145b73d80bacac8",
          "grade": true,
          "grade_id": "cell-a64a178c1840c6e5",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This cell contains hidden test cases that will be evaluated after submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bacec3dd-cc2f-409d-b36c-6ed1411a4f02",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bacec3dd-cc2f-409d-b36c-6ed1411a4f02",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "60281fcdc29d77d7f75151cccc9d35df",
          "grade": true,
          "grade_id": "cell-7824bdcc6d6eda6a",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
