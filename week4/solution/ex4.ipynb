{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "05e27d7a",
      "metadata": {
        "id": "05e27d7a"
      },
      "source": [
        "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution.**  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37eb1f5d-41ec-4060-9f20-c28c37ec2b05",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "37eb1f5d-41ec-4060-9f20-c28c37ec2b05",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "79b7b4fe88b5ce1455d79a90e751565c",
          "grade": false,
          "grade_id": "cell-341078e8dde7e0e6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Exercise 4: Text Generation using LSTM\n",
        "\n",
        "### Objective\n",
        "In this assignment, you will implement a character-level text generation model using Long Short-Term Memory (LSTM) networks in PyTorch. The goal is to understand how LSTMs work for sequential data and how to train them effectively to generate new text based on an input sequence.\n",
        "\n",
        "You will follow the steps below:\n",
        "1. Load and preprocess a text dataset\n",
        "2. Character-level encoding by constructing the vocabulary and dictionary (2 points)\n",
        "3. Batch generation for training (6 points)\n",
        "4. Defining the character-level LSTM model (6 points)\n",
        "5. Training loop (3 points)\n",
        "6. Text generation using the trained model (3 points)\n",
        "\n",
        "\n",
        "**IMPORTANT NOTE**: Kindly remove the line \"raise NotImplementedError()\" from all cells wherever present.\n",
        "\n",
        "**Deliverables**:\n",
        "\n",
        "Submit the completed notebook (ex4.ipynb) and your trained model (best_model.pth). Do not change the name of the notebook file. It may result in 0 points for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e84bca02-56b3-446d-a2b3-a35bcdfc0d34",
      "metadata": {
        "id": "e84bca02-56b3-446d-a2b3-a35bcdfc0d34"
      },
      "outputs": [],
      "source": [
        "skip_training = False   # You can set it to True if you want to run inference on your trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e388c081-faa7-450c-9bb9-750fee6db9fa",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e388c081-faa7-450c-9bb9-750fee6db9fa",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f01972364c7675fff5d2dcd373349ec0",
          "grade": true,
          "grade_id": "cell-cfe23220f649bad0",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598c309a-1cb0-490b-8eb3-8c0ad9441d32",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "598c309a-1cb0-490b-8eb3-8c0ad9441d32",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "31eca098bf738ae87c126a8dd4a7b28d",
          "grade": false,
          "grade_id": "cell-22ac2dd116c986e6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e1996f4f-0290-4073-8f7a-47a30ea8d7c5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e1996f4f-0290-4073-8f7a-47a30ea8d7c5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fd6566ff374217876145e2ff34a74cae",
          "grade": false,
          "grade_id": "cell-8d754ffa5b6f36a3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a87cd37b-67f9-4fc0-b745-fca21d3c36db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "a87cd37b-67f9-4fc0-b745-fca21d3c36db",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fd144beaefb5f28eeebf711a429118dc",
          "grade": false,
          "grade_id": "cell-1a5c0ed78b5fe3d7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cd382d1b-c2a7-48a6-a90c-fb43d021c325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66259c62-2979-4b9f-ad1f-bbc76e5bbd29",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "66259c62-2979-4b9f-ad1f-bbc76e5bbd29",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e3f0f3acc67cf1db9005351e946bee4c",
          "grade": false,
          "grade_id": "cell-918dec79c2334ca9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 1. Load and Preprocess the Text Dataset\n",
        "\n",
        "We will be using *Alice's Adventures in Wonderland* by Lewis Carroll as our dataset. You can download it from [Project Gutenberg](https://www.gutenberg.org/):\n",
        "\n",
        "[Alice's Adventures in Wonderland by Lewis Carroll (Project Gutenberg Page)](https://www.gutenberg.org/ebooks/11) \\\n",
        "[Direct Text File Download](https://www.gutenberg.org/files/11/11-0.txt)\n",
        "\n",
        "We’ve chosen Alice's Adventures in Wonderland as a relatively small text to make training still manageable on a CPU. However, you are highly encouraged to explore other texts from Project Gutenberg or other public domain sources.\n",
        "\n",
        "This section contains the following steps:\n",
        "1. Load the dataset into Python\n",
        "2. Remove metadata to focus on the main part of the text\n",
        "3. Clean the text by removing special characters and converting it to lowercase\n",
        "   \n",
        "The goal is to preprocess the dataset by filtering out any metadata that is not part of the text, converting the text to lowercase, and removing unnecessary punctuation. We will also build a dictionary to map each unique character to a unique integer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07811d19-7527-445c-9fa3-0a423ab22f77",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "07811d19-7527-445c-9fa3-0a423ab22f77",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f06c5177f35d548718750d8eed3f43c2",
          "grade": false,
          "grade_id": "cell-0e71c281d4cb17c5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### 1.1. Load the Dataset\n",
        "\n",
        "We start by loading the text dataset into Python. The dataset should be a plain text file. The first step is to load and inspect a small portion of the raw text to understand its structure to identify any unwanted metadata or special characters that should be removed during preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "39b5e555-045e-4f8f-8150-0f0b69cce823",
      "metadata": {
        "id": "39b5e555-045e-4f8f-8150-0f0b69cce823"
      },
      "outputs": [],
      "source": [
        "txt_path = '/content/alice.txt' # replace 'alice.txt' with your txt path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "06f4c60b-dd73-4306-9ac2-b9e89a1e3c7a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "06f4c60b-dd73-4306-9ac2-b9e89a1e3c7a",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c946acfea2f9f73ce6a0dc7d293e7162",
          "grade": true,
          "grade_id": "cell-6e62ac8916a2b985",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f1536e4c-1437-432f-b332-ee9e366f34b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1536e4c-1437-432f-b332-ee9e366f34b7",
        "outputId": "f3522775-7798-4474-bc18-1ec0adfcb5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===First 1500 characters before any processing:\n",
            "\n",
            "\n",
            "﻿The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "Title: Alice's Adventures in Wonderland\n",
            "\n",
            "Author: Lewis Carroll\n",
            "\n",
            "Release date: June 27, 2008 [eBook #11]\n",
            "                Most recently updated: October 21, 2024\n",
            "\n",
            "Language: English\n",
            "\n",
            "Credits: Arthur DiBianca and David Widger\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n",
            "[Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Alice’s Adventures in Wonderland\n",
            "\n",
            "by Lewis Carroll\n",
            "\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Contents\n",
            "\n",
            " CHAPTER I.     Down the Rabbit-Hole\n",
            " CHAPTER II.    The Pool of Tears\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
            " CHAPTER V.     Advice from a Caterpillar\n",
            " CHAPTER VI.    Pig and Pepper\n",
            " CHAPTER VII.   A Mad Tea-Party\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
            " CHAPTER IX.    The Mock Turtle’s Story\n",
            " CHAPTER X.     The Lobster Quadrille\n",
            " CHAPTER XI.    Who Stole the Tarts?\n",
            " CHAPTER XII.   Alice’s Evidence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on \n",
            "\n",
            "\n",
            "\n",
            "===Ending characters before any processing:\n",
            "\n",
            "\n",
            "would, in the after-time, be herself a grown woman; and how she would\n",
            "keep, through all her riper years, the simple and loving heart of her\n",
            "childhood: and how she would gather about her other little children,\n",
            "and make _their_ eyes bright and eager with many a strange tale,\n",
            "perhaps even with the dream of Wonderland of long ago: and how she\n",
            "would feel with all their simple sorrows, and find a pleasure in all\n",
            "their simple joys, remembering her own child-life, and the happy summer\n",
            "days.\n",
            "\n",
            "THE END\n",
            "\n",
            "\n",
            "\n",
            "*** END OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n",
            "\n",
            "\n",
            "    \n",
            "\n",
            "Updated editions will replace the previous one—the old editions will\n",
            "be renamed.\n",
            "\n",
            "Creating the works from print editions not protected by U.S. copyright\n",
            "law means that no one owns a United States copyright in these works,\n",
            "so the Foundation (and you!) can copy and distribute it in the United\n",
            "States without permission and without paying copyright\n",
            "royalties. Special rules, set forth in the General Terms of Use part\n",
            "of this license, apply to copying and distributing Project\n",
            "Gutenberg™ electronic works to protect the PROJECT GUTENBERG™\n",
            "concept and trademark. Project Gutenberg is a registered trademark,\n",
            "and may not be used if you charge for an eBook, except by following\n",
            "the terms of the trademark license, including paying royalties for use\n",
            "of the Project Gutenberg trademark. If you do not charge anything for\n",
            "copies of this eBook, complying with the trademark license is very\n",
            "easy. You may use this eBook for nearly any purpose such as creation\n",
            "of derivative works, reports, performances and research. Project\n",
            "Gutenberg eBooks may be modified and printed and given away—you may\n",
            "do practically ANYTHING in the United States with eBooks not protected\n",
            "by U.S. copyright law. Redistribution is subject to the trademark\n",
            "license, especially commercial redistribution.\n",
            "\n",
            "\n",
            "START: FULL LICENSE\n",
            "\n",
            "THE FULL PROJECT GUTENBERG LICENSE\n",
            "\n",
            "PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n",
            "\n",
            "To protect the Project Gutenberg\n"
          ]
        }
      ],
      "source": [
        "#with open(txt_path, 'r') as file:\n",
        "with open(txt_path, 'r', encoding = 'utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "####\n",
        "print('===First 1500 characters before any processing:\\n\\n')\n",
        "print(raw_text[:1500])\n",
        "\n",
        "print('\\n\\n\\n===Ending characters before any processing:\\n')\n",
        "print(raw_text[-19000:-17000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32bfef24-0842-427c-aa49-2f01f36b8ff0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "32bfef24-0842-427c-aa49-2f01f36b8ff0",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0496274c867d0502435d160f36c14010",
          "grade": false,
          "grade_id": "cell-100dde1181de781c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### 1.2. Remove Metadata and Focus on the Main Text\n",
        "\n",
        "Text files may contain introductary or ending metadata such as copyright information. We want to focus only on the main body of the text. For Alice's Adventures in Wonderland, we remove everything before the first chapter and after the Project Gutenberg closing markers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6fce77de-0ccd-40b1-ac63-3ebff4c9f8ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fce77de-0ccd-40b1-ac63-3ebff4c9f8ab",
        "outputId": "a8462099-e990-4015-8864-fbc19fdf3992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===Text after removing metadata:\n",
            "\n",
            "CHAPTER I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
            "making a daisy-chain would be worth the trouble of getting up and\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
            "close by her.\n",
            "\n",
            "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
            "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
            "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
            "it occurred to her that she ought to have wondered at this, but at the\n",
            "time it all seemed quite natural); but when the Rabbit actually _took a\n",
            "watch out of its waistcoat-pocket_, and looked at it, and then hurried\n",
            "on, Alice started to her feet, for it flashed across her mind that she\n",
            "had never before seen a rabbit with either a waistcoat-pocket, or a\n",
            "watch to take out of it, and burning with curiosity, she ran across the\n",
            "field after it, and fortunately was just in time to see it pop down a\n",
            "large rabbit-hole under the hedge.\n",
            "\n",
            "In another moment down went Alice after it, never once considering how\n",
            "in the world she was to get out again.\n",
            "\n",
            "The rabbit-hole wen\n"
          ]
        }
      ],
      "source": [
        "# For this example, we are removing everything before 'CHAPTER I.\\nDown the Rabbit-Hole'\n",
        "# and after the end marker\n",
        "start_index = raw_text.find('CHAPTER I.\\nDown the Rabbit-Hole')\n",
        "\n",
        "end_index = raw_text.find('*** END OF THE PROJECT GUTENBERG') # closing markers of Project Gutenberg\n",
        "\n",
        "trimmed_text = raw_text[start_index:end_index]\n",
        "\n",
        "print('===Text after removing metadata:\\n')\n",
        "print(trimmed_text[:1500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607662e4-b640-4aeb-9bf6-3ee5fca989ab",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "607662e4-b640-4aeb-9bf6-3ee5fca989ab",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d9b89e7035c671d5a4ac7fbed1f53d92",
          "grade": false,
          "grade_id": "cell-ee8493145fa5be43",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### 1.3. Clean the Text\n",
        "\n",
        "Next, we preprocess the text by removing any special characters, leaving only alphanumeric characters, and normalizing spaces. We also convert all text to lowercase to standardize the format. This helps the model learn without case sensitivity or irrelevant symbols.\n",
        "\n",
        "##### Steps to follow:\n",
        "##### 1. Convert text to lowercase:\n",
        "First, convert the text to lowercase to avoid treating uppercase and lowercase letters as different characters.\n",
        "##### 2. Remove special characters:\n",
        "Then, you need to remove any character that is not a letter `(a-z)`, a number `(0-9)`, or a space `\\s`.\n",
        "##### 3. Handling double spaces:\n",
        "After removing characters, there may be extra spaces in the text. Make sure that sequences of multiple spaces are reduced to just a single space.\n",
        "\n",
        "**Hint:** You can use the `re.sub()` method of regular expressions library `re` to replace the patterns. In this case, you will be replacing non-alphanumeric characters (`[^a-z0-9\\s]`) with spaces, and whitespace sequences (`\\s+`) with a single space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f910909e-acba-45f9-ac49-41c0d3432129",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "f910909e-acba-45f9-ac49-41c0d3432129",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1f65b86777609239806c103b9d5d4933",
          "grade": false,
          "grade_id": "cell-6e2568640b9031df",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "d60a7ea7-2c6c-40fb-a5a5-c6e69628d813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after cleaning and converting to lowercase:\n",
            "\n",
            "chapter i down the rabbit hole alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought alice without pictures or conversations so she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid whether the pleasure of making a daisy chain would be worth the trouble of getting up and picking the daisies when suddenly a white rabbit with pink eyes ran close by her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it occurred to her that she ought to have wondered at this but at the time it all seemed quite natural but when the rabbit actually took a watch out of its waistcoat pocket and looked at it and t\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text by i. converting it to lowercase,\n",
        "    ii. removing non-alphanumeric characters (except spaces),\n",
        "    iii. and normalizing spaces.\n",
        "\n",
        "    Args:\n",
        "    text -- The raw input text as a string\n",
        "\n",
        "    Returns:\n",
        "    cleaned_text -- The processed text where all the preprocessing steps are applied\n",
        "    \"\"\"\n",
        "    # 1. Convert text to lowercase\n",
        "    # 2. Remove special characters\n",
        "    # 3. Remove double spaces\n",
        "\n",
        "    # 1. Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Replace any character that is NOT a letter, number, or whitespace with a space\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "\n",
        "    # 3. Replace one or more whitespace characters with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Optional: strip leading/trailing spaces (good practice)\n",
        "    cleaned_text = text.strip()\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = preprocess_text(trimmed_text)\n",
        "print('Text after cleaning and converting to lowercase:\\n')\n",
        "print(cleaned_text[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4c0d3467-77ae-49f0-b2ea-01ad6d2cc41a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "de4559b9579dfbcff9fa608aeafb34b1",
          "grade": true,
          "grade_id": "cell-d5172a46eaad5be8",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c0d3467-77ae-49f0-b2ea-01ad6d2cc41a",
        "outputId": "8dcf02f1-51b7-4911-a62f-f6cd3c96fcce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text cleaning test passed successfully!\n"
          ]
        }
      ],
      "source": [
        "def test_text_cleaning(text, cleaned_text):\n",
        "    \"\"\"\n",
        "    Visible test case for verifying text cleaning function.\n",
        "    Collects all errors instead of stopping at the first failure.\n",
        "    \"\"\"\n",
        "\n",
        "    errors = []\n",
        "    all_tests_successful = True\n",
        "\n",
        "    # Test 1: Check if the text length is reduced\n",
        "    if not (len(cleaned_text) < len(text)):\n",
        "        all_tests_successful = False\n",
        "        errors.append(\"Task 1: Visible test: Text cleaning: The cleaned text should be shorter than the original raw text.\")\n",
        "\n",
        "    # Test 2: All characters should be lowercase\n",
        "    if not cleaned_text.islower():\n",
        "        all_tests_successful = False\n",
        "        errors.append(\"Text cleaning: The cleaned text is not fully lowercase.\")\n",
        "\n",
        "    # Test 3: Ensure all special characters are removed\n",
        "    if not all(char.isalnum() or char == ' ' for char in cleaned_text):\n",
        "        all_tests_successful = False\n",
        "        errors.append(\"Text cleaning: Special characters are still present in the cleaned text.\")\n",
        "\n",
        "    # Test 4: Ensure no consecutive spaces exist\n",
        "    if \"  \" in cleaned_text:\n",
        "        all_tests_successful = False\n",
        "        errors.append(\"Text cleaning: Multiple consecutive spaces detected in the cleaned text.\")\n",
        "\n",
        "    # --- Report results ---\n",
        "    if errors:\n",
        "        feedback_txt.append(\"Task 1: Visible test: \")\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError(\"\\n\".join(errors))\n",
        "    elif all_tests_successful:\n",
        "        print(\"Text cleaning test passed successfully!\")\n",
        "\n",
        "# Run visible test\n",
        "test_text_cleaning(raw_text, cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b05133-4168-459e-8e36-2ec88d2ad51d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e7b05133-4168-459e-8e36-2ec88d2ad51d",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6a920af1dea881be0da01e7f1013c338",
          "grade": false,
          "grade_id": "cell-ae2039f6bc91f976",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2. Character-Level Encoding (2 points)\n",
        "\n",
        "In this step, we convert the cleaned text into a format that the model can understand. Since we are working with character-level encoding, each individual character will be treated as a token. This allows the LSTM to learn patterns at the character level and generate text one character at a time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a820382-8e61-4d1c-a0b5-588d81b58320",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4a820382-8e61-4d1c-a0b5-588d81b58320",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bc5b454add8e5408a13c0010452fe78a",
          "grade": false,
          "grade_id": "cell-02de74b25a6da964",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### 2.1 Character-Level Vocabulary\n",
        "\n",
        "In this section, you will create a vocabulary of from the cleaned text and map each element to a unique integer.\n",
        "\n",
        "##### Steps to follow:\n",
        "##### 1. Create a vocabulary of unique characters:\n",
        "First, you need to extract all the unique characters from the cleaned text. You can use Python's built-in `set()` function to find unique characters. For consistency and easier debugging, you should also sort the unique characters.\n",
        "##### 2. Construct mapping between characters and integers:\n",
        "Once you have the unique characters, create a dictionary `char_to_int` that maps each character to a unique integer to represent each character during training. You should also create a reverse mapping `int_to_char` that maps integers back to characters to be used when decoding the text later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "64011399-7d6d-4cb3-89e8-fa65653a5a72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "64011399-7d6d-4cb3-89e8-fa65653a5a72",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "02558911a39ac511871aa6e2c60fe160",
          "grade": false,
          "grade_id": "cell-d2e35b87ae8fa70e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "9bb2c08f-32c3-4213-dad6-88987475634d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character to Integer Mapping:\n",
            "' ' : 0\n",
            "'a' : 1\n",
            "'b' : 2\n",
            "'c' : 3\n",
            "'d' : 4\n",
            "'e' : 5\n",
            "'f' : 6\n",
            "'g' : 7\n",
            "'h' : 8\n",
            "'i' : 9\n",
            "'j' : 10\n",
            "'k' : 11\n",
            "'l' : 12\n",
            "'m' : 13\n",
            "'n' : 14\n",
            "'o' : 15\n",
            "'p' : 16\n",
            "'q' : 17\n",
            "'r' : 18\n",
            "'s' : 19\n",
            "'t' : 20\n",
            "'u' : 21\n",
            "'v' : 22\n",
            "'w' : 23\n",
            "'x' : 24\n",
            "'y' : 25\n",
            "'z' : 26\n"
          ]
        }
      ],
      "source": [
        "def create_char_mappings(cleaned_text):\n",
        "    \"\"\"\n",
        "    Creates character-to-integer and integer-to-character mappings from the cleaned text.\n",
        "\n",
        "    Args:\n",
        "    cleaned_text -- The cleaned input text as a string\n",
        "\n",
        "    Returns:\n",
        "    char_to_int -- A dictionary mapping each unique character to an integer\n",
        "    int_to_char -- A dictionary mapping each integer back to its corresponding character\n",
        "    \"\"\"\n",
        "    # Get all unique characters in the text\n",
        "    unique_chars = sorted(set(cleaned_text))\n",
        "\n",
        "    # Create character → integer mapping\n",
        "    char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "\n",
        "    # Create integer → character mapping\n",
        "    int_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
        "\n",
        "    return char_to_int, int_to_char\n",
        "\n",
        "char_to_int, int_to_char = create_char_mappings(cleaned_text)\n",
        "print('Character to Integer Mapping:')\n",
        "for char, idx in list(char_to_int.items()):\n",
        "    print(f\"'{char}' : {idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33f28d5-de30-4ffc-99cd-ee43a15926f8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c33f28d5-de30-4ffc-99cd-ee43a15926f8",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5bd13e000e8dca85b863d1014c17d836",
          "grade": false,
          "grade_id": "cell-d3a464179b6eeffb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### 2.2 Encode the Text into Integers\n",
        "\n",
        "During training, the model will use the encoded representation of the cleaned text as the input. In this section, you need to convert each character in the cleaned text to its corresponding integer using `char_to_int` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "af1aa404-54f9-4481-9e83-183f727b654e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "af1aa404-54f9-4481-9e83-183f727b654e",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8846a46a9d6bebdd3465d529c3f61cc2",
          "grade": false,
          "grade_id": "cell-82f52b40eea42a5c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "7b46dc23-7a3a-44bb-aba1-b2cf79eda6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total encoded characters: 135001\n",
            "First 100 encoded characters:\n",
            "[ 3  8  1 16 20  5 18  0  9  0  4 15 23 14  0 20  8  5  0 18  1  2  2  9\n",
            " 20  0  8 15 12  5  0  1 12  9  3  5  0 23  1 19  0  2  5  7  9 14 14  9\n",
            " 14  7  0 20 15  0  7  5 20  0 22  5 18 25  0 20  9 18  5  4  0 15  6  0\n",
            " 19  9 20 20  9 14  7  0  2 25  0  8  5 18  0 19  9 19 20  5 18  0 15 14\n",
            "  0 20  8  5]\n"
          ]
        }
      ],
      "source": [
        "def encode_text(cleaned_text, char_to_int):\n",
        "    \"\"\"\n",
        "    Encodes the cleaned text into an array of integers.\n",
        "\n",
        "    Args:\n",
        "    cleaned_text -- The cleaned input text as a string\n",
        "    char_to_int -- Characters to integer mapping\n",
        "\n",
        "    Returns:\n",
        "    encoded_chars -- Numpy array of integers representing the encoded characters from the text\n",
        "    \"\"\"\n",
        "    # Convert each character in the text to its corresponding integer\n",
        "    encoded_list = [char_to_int[char] for char in cleaned_text]\n",
        "\n",
        "    # Convert the Python list to a NumPy array\n",
        "    encoded_chars = np.array(encoded_list, dtype=np.int64)\n",
        "\n",
        "    return encoded_chars\n",
        "\n",
        "encoded_chars = encode_text(cleaned_text, char_to_int)\n",
        "print(f\"Total encoded characters: {len(encoded_chars)}\")\n",
        "print('First 100 encoded characters:')\n",
        "print(encoded_chars[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0741605f-8d14-45b5-97c5-c66f58938c13",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c8cd79f4d8b44d2cb804b30022051853",
          "grade": true,
          "grade_id": "cell-4f0945e255cdf8be",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0741605f-8d14-45b5-97c5-c66f58938c13",
        "outputId": "6965d9cb-8822-4b74-9497-dd86f6258978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character encoding length test passed successfully!\n"
          ]
        }
      ],
      "source": [
        "### BEGIN VISIBLE TESTS\n",
        "def test_character_encoding_length(cleaned_text, encoded_chars, char_to_int, int_to_char):\n",
        "    \"\"\"\n",
        "    Visible test case for verifying character encoding length and consistency.\n",
        "    Collects all errors and writes them to feedback_txt.\n",
        "    \"\"\"\n",
        "\n",
        "    errors = []\n",
        "    all_tests_successful = True\n",
        "\n",
        "    # ---- Test 1: Dictionary size match ----\n",
        "    if len(char_to_int) != len(int_to_char):\n",
        "        all_tests_successful = False\n",
        "        errors.append(\n",
        "            \"Character encoding: char_to_int and int_to_char dictionaries should have the same length.\"\n",
        "        )\n",
        "\n",
        "    # ---- Test 2: Encoded text length match ----\n",
        "    if len(encoded_chars) != len(cleaned_text):\n",
        "        all_tests_successful = False\n",
        "        errors.append(\n",
        "            \"Character encoding: The length of encoded_chars should match the length of cleaned_text.\"\n",
        "        )\n",
        "\n",
        "    # ---- Test 3: Round-trip decoding correctness ----\n",
        "    try:\n",
        "        decoded_text = ''.join([int_to_char[i] for i in encoded_chars])\n",
        "        if decoded_text != cleaned_text:\n",
        "            all_tests_successful = False\n",
        "            errors.append(\n",
        "                \"Character encoding: Decoded text does not match the original cleaned text.\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        all_tests_successful = False\n",
        "        errors.append(\n",
        "            f\"Character encoding: Error during decoding — {str(e)}\"\n",
        "        )\n",
        "\n",
        "    # ---- Report results ----\n",
        "    if errors:\n",
        "        feedback_txt.append(\"Task 2: Visible test: -1 points from the following:\")\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError(\"\\n\".join(errors))\n",
        "    elif all_tests_successful:\n",
        "        print(\"Character encoding length test passed successfully!\")\n",
        "\n",
        "# Run visible test\n",
        "test_character_encoding_length(cleaned_text, encoded_chars, char_to_int, int_to_char)\n",
        "### END VISIBLE TESTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a9c57403-e236-4776-8403-8de6b07487f4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a9c57403-e236-4776-8403-8de6b07487f4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0b0f818f08fd374f0afe3baa2128a6eb",
          "grade": true,
          "grade_id": "cell-67bcfe87b5f67411",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6c011421-7952-4c8d-8217-55c04e7d608f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6c011421-7952-4c8d-8217-55c04e7d608f",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "641539c878b4e707dad47e7bacdd6b7b",
          "grade": false,
          "grade_id": "cell-f6f0731068619a8c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 3. Batch Generation for Training (6 points)\n",
        "In this step, you will implement the function `get_batches()` that splits the encoded data into smaller batches for training. Each batch will have input sequences `x` and target sequences `y` where `y` is `x` shifted by one position.  This means that the model is trained to generate the next character in the sequence based on the previous ones.\n",
        "\n",
        "##### Steps to follow:\n",
        "##### 1. Handle step_size:\n",
        "The step size determines how much the window moves across the data after each sequence is generated. If `step_size` is not provided, it is set to `seq_length`. This means the sequences will not overlap. A smaller `step_size` allows for overlapping sequences.\n",
        "\n",
        "##### 2. Calculate the number of batches:\n",
        "When calculating how many batches you can generate from the input data, there are some key factors to consider:\n",
        "1. Sequence Length: Each input sequence in a batch will contain a specific number of tokens (representing the characters). The longer your sequence length is, the fewer total sequences you can generate from the input data.\n",
        "2. Step Size: A smaller step size results in more overlap between sequences and it allows you to generate more sequences from the same input. If the step size is larger, there will be less overlap (or none at all if step size equals sequence length), leading to fewer sequences in total.\n",
        "3. Batch Size: Once you generate sequences, you need to group them into batches for efficient training. The batch size defines how many sequences are grouped in each batch. A larger batch size means fewer batches because more sequences are grouped together in each batch.\n",
        "\n",
        "Make sure to generate full number of batches.\n",
        "\n",
        "##### 3. Trim the input array:\n",
        "If the input data does not perfectly divide into batches, trim the array so it contains only full batches. Avoid having incomplete sequences at the end.\n",
        "\n",
        "##### 4. Generate batches:\n",
        "Use nested loops to generate batches:\n",
        "- `x` will be the input sequence of length `seq_length`.\n",
        "- `y` will be target sequence, which is `x` shifted by one position (token).\n",
        "\n",
        "##### 5. Store and return batches:\n",
        "Store the input and target sequences in separate arrays (`x_batches` and `y_batches`) and return them as NumPy arrays to be used in training.\n",
        "\n",
        "**Expected Shape**:\n",
        "- Each batch in `x_batches` and `y_batches` should have the shape `(batch_size, seq_length)`.\n",
        "- The returned `x_batches` and `y_batches` should be NumPy arrays with shapes `(num_batches, batch_size, seq_length)`.\n",
        "\n",
        "**Important Notes**:\n",
        "- Support for both overlapping and non-overlapping sequences using `step_size`.\n",
        "- Handle edge cases where the data does not fit perfectly into full batches.\n",
        "- Think about how you are generating both the `x` and `y` sequences. Their size should match but `y` should always be one token ahead of `x`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "afe5f57f-0bce-428d-8d46-67d71def6237",
      "metadata": {
        "deletable": false,
        "id": "afe5f57f-0bce-428d-8d46-67d71def6237",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d25f3054980fd9d9eec99685ebb48557",
          "grade": false,
          "grade_id": "cell-0c4e1828e13b2561",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ef89fe-fb69-4cef-d939-361792ced226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches: 702\n",
            "Batch shape: (batch_size=64, seq_length=100)\n",
            "x_batches shape: (702, 64, 100)\n",
            "y_batches shape: (702, 64, 100)\n",
            "\n",
            "First sequence check:\n",
            "x[0]: [ 3  8  1 16 20  5 18  0  9  0] → ... → [14  0 20  8  5]\n",
            "y[0]: [ 8  1 16 20  5 18  0  9  0  4] → ... → [ 0 20  8  5  0]\n",
            "Correct shift? False\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_batches(encoded_chars, batch_size, seq_length, step_size=None):\n",
        "    \"\"\"\n",
        "    Generates batches of input (x) and target (y) sequences from encoded text.\n",
        "\n",
        "    Args:\n",
        "        encoded_chars: 1D numpy array of encoded integers (the full text)\n",
        "        batch_size:    Number of sequences per batch\n",
        "        seq_length:    Length of each sequence\n",
        "        step_size:     How many positions to move forward for the next sequence.\n",
        "                       If None, defaults to seq_length (non-overlapping)\n",
        "\n",
        "    Returns:\n",
        "        x_batches: np.array of shape (num_batches, batch_size, seq_length)\n",
        "        y_batches: np.array of shape (num_batches, batch_size, seq_length)\n",
        "    \"\"\"\n",
        "    if step_size is None:\n",
        "        step_size = seq_length  # Default: no overlap\n",
        "\n",
        "    # Total length needed: we need seq_length + 1 because y is shifted by 1\n",
        "    total_length_needed = seq_length + 1\n",
        "\n",
        "    # Find how many complete sequences we can make with the given step_size\n",
        "    n_sequences = 0\n",
        "    pos = 0\n",
        "    while pos + total_length_needed <= len(encoded_chars):\n",
        "        n_sequences += 1\n",
        "        pos += step_size\n",
        "\n",
        "    # Total number of sequences we can generate\n",
        "    if n_sequences == 0:\n",
        "        raise ValueError(\"Text is too short for the given seq_length and step_size.\")\n",
        "\n",
        "    # Trim to only full batches\n",
        "    total_sequences_in_full_batches = (n_sequences // batch_size) * batch_size\n",
        "    if total_sequences_in_full_batches == 0:\n",
        "        raise ValueError(f\"Cannot form even one full batch with batch_size={batch_size}. \"\n",
        "                         f\"Only {n_sequences} sequences available.\")\n",
        "\n",
        "    # We'll collect all valid sequences first\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "\n",
        "    pos = 0\n",
        "    count = 0\n",
        "    while count < total_sequences_in_full_batches:\n",
        "        if pos + total_length_needed <= len(encoded_chars):\n",
        "            x_seq = encoded_chars[pos : pos + seq_length]\n",
        "            y_seq = encoded_chars[pos + 1 : pos + seq_length + 1]\n",
        "\n",
        "            x_list.append(x_seq)\n",
        "            y_list.append(y_seq)\n",
        "\n",
        "            count += 1\n",
        "        pos += step_size\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    x = np.array(x_list)  # Shape: (total_sequences_in_full_batches, seq_length)\n",
        "    y = np.array(y_list)\n",
        "\n",
        "    # Reshape into batches\n",
        "    num_batches = total_sequences_in_full_batches // batch_size\n",
        "\n",
        "    x_batches = x.reshape(num_batches, batch_size, seq_length)\n",
        "    y_batches = y.reshape(num_batches, batch_size, seq_length)\n",
        "\n",
        "    return x_batches, y_batches\n",
        "\n",
        "\n",
        "# === Example Usage ===\n",
        "batch_size = 64\n",
        "seq_length = 100\n",
        "step_size = 3  # Small step → lots of overlap → more training data (good!)\n",
        "\n",
        "x_batches, y_batches = get_batches(encoded_chars, batch_size, seq_length, step_size)\n",
        "\n",
        "print(f\"Number of batches: {x_batches.shape[0]}\")\n",
        "print(f\"Batch shape: (batch_size={x_batches.shape[1]}, seq_length={x_batches.shape[2]})\")\n",
        "print(f\"x_batches shape: {x_batches.shape}\")\n",
        "print(f\"y_batches shape: {y_batches.shape}\")\n",
        "\n",
        "# Verify that y is indeed x shifted by one\n",
        "print(\"\\nFirst sequence check:\")\n",
        "print(\"x[0]:\", x_batches[0, 0, :10], \"→ ... →\", x_batches[0, 0, -5:])\n",
        "print(\"y[0]:\", y_batches[0, 0, :10], \"→ ... →\", y_batches[0, 0, -5:])\n",
        "print(\"Correct shift?\" , np.all(y_batches[0, 0] == np.roll(x_batches[0, 0], -1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "558ba631-68f6-4925-b744-8d2cf3adcade",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "10ae780bd5c8f409e19cc024bb28f1da",
          "grade": true,
          "grade_id": "cell-d799842e1768605a",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "558ba631-68f6-4925-b744-8d2cf3adcade",
        "outputId": "487e1cea-2b57-4be4-979a-4cfcd0302888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All visible batch generation tests passed successfully!\n"
          ]
        }
      ],
      "source": [
        "### BEGIN VISIBLE TESTS\n",
        "def test_batch_generation_shape_no_overlap(encoded_chars):\n",
        "    \"\"\"\n",
        "    Visible test case for verifying batch generation when there is no overlap between sequences.\n",
        "    Collects all errors and writes them to feedback_txt.\n",
        "    \"\"\"\n",
        "\n",
        "    errors = []\n",
        "    all_tests_successful = True\n",
        "\n",
        "    try:\n",
        "        x_batches, y_batches = get_batches(encoded_chars, batch_size=64, seq_length=100)\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Error: get_batches() raised an exception — {str(e)}\")\n",
        "        all_tests_successful = False\n",
        "        x_batches, y_batches = None, None\n",
        "\n",
        "    #  Test 1: Matching batch counts\n",
        "    if x_batches is not None and y_batches is not None:\n",
        "        if len(x_batches) != len(y_batches):\n",
        "            all_tests_successful = False\n",
        "            errors.append(\n",
        "                \"Batch generation: The number of x_batches and y_batches should be the same.\"\n",
        "            )\n",
        "\n",
        "    # Test 2: Check shapes\n",
        "    expected_shape = (21, 64, 100)\n",
        "    if x_batches is not None:\n",
        "        if x_batches.shape != expected_shape:\n",
        "            all_tests_successful = False\n",
        "            errors.append(\n",
        "                f\"Batch generation: Expected x_batches shape {expected_shape}, but got {x_batches.shape}.\"\n",
        "            )\n",
        "    if y_batches is not None:\n",
        "        if y_batches.shape != expected_shape:\n",
        "            all_tests_successful = False\n",
        "            errors.append(\n",
        "                f\"Batch generation: Expected y_batches shape {expected_shape}, but got {y_batches.shape}.\"\n",
        "            )\n",
        "\n",
        "    # Report results\n",
        "    if errors:\n",
        "        feedback_txt.append(\"Task 3: Visible test: -2 points from the following:\")\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError(\"\\n\".join(errors))\n",
        "    elif all_tests_successful:\n",
        "        print(\"All visible batch generation tests passed successfully!\")\n",
        "\n",
        "# Run the visible test\n",
        "test_batch_generation_shape_no_overlap(encoded_chars)\n",
        "### END VISIBLE TESTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bec5530e-581d-41de-9736-32f28b4848f8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aec29aabc23b14348c9e703633c16399",
          "grade": true,
          "grade_id": "cell-f14a631f73dd6ee8",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bec5530e-581d-41de-9736-32f28b4848f8",
        "outputId": "5e115205-4b63-4e5a-e283-21b17ce822fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All visible batch generation (overlap) tests passed successfully!\n"
          ]
        }
      ],
      "source": [
        "### BEGIN VISIBLE TESTS\n",
        "def test_batch_generation_shape_overlap(encoded_chars):\n",
        "    \"\"\"\n",
        "    Visible test case for verifying batch generation when sequences overlap (step_size != seq_length).\n",
        "    Collects all assertion failures into feedback_txt.\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    all_tests_successful = True\n",
        "\n",
        "    # ---- Run the student's batch generator ----\n",
        "    try:\n",
        "        x_batches, y_batches = get_batches(encoded_chars, batch_size=64, seq_length=100, step_size=50)\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Error: get_batches() raised an exception — {str(e)}\")\n",
        "        all_tests_successful = False\n",
        "        x_batches, y_batches = None, None\n",
        "\n",
        "    # ---- Test 1: Ensure same number of x/y batches ----\n",
        "    if x_batches is not None and y_batches is not None:\n",
        "        if len(x_batches) != len(y_batches):\n",
        "            all_tests_successful = False\n",
        "            errors.append(\n",
        "                \"Batch generation (overlap): The number of x_batches and y_batches should be the same.\"\n",
        "            )\n",
        "\n",
        "    # ---- Test 2: Validate shapes ----\n",
        "    expected_shape = (42, 64, 100)\n",
        "    if x_batches is not None:\n",
        "        if x_batches.shape != expected_shape:\n",
        "            all_tests_successful = False\n",
        "            errors.append(\n",
        "                f\"Batch generation (overlap): Expected x_batches shape {expected_shape}, but got {x_batches.shape}.\"\n",
        "            )\n",
        "    if y_batches is not None:\n",
        "        if y_batches.shape != expected_shape:\n",
        "            all_tests_successful = False\n",
        "            errors.append(\n",
        "                f\"Batch generation (overlap): Expected y_batches shape {expected_shape}, but got {y_batches.shape}.\"\n",
        "            )\n",
        "\n",
        "    # ---- Report results ----\n",
        "    if errors:\n",
        "        feedback_txt.append(\"Task 3: Visible test: -2 points from the following:\")\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError(\"\\n\".join(errors))\n",
        "    elif all_tests_successful:\n",
        "        print(\"All visible batch generation (overlap) tests passed successfully!\")\n",
        "\n",
        "# Run the visible test\n",
        "test_batch_generation_shape_overlap(encoded_chars)\n",
        "### END VISIBLE TESTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c7e33b6e-b7f4-45c7-b3bc-14c80d71c4ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "c7e33b6e-b7f4-45c7-b3bc-14c80d71c4ef",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4606767d6cce6b002bc2159704f63d3c",
          "grade": false,
          "grade_id": "cell-a063285239364470",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "d4f6376d-5267-4e49-ab57-8d27c1b394b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Displaying a Single Batch\n",
            "==================================================\n",
            "[made her f]  -->  [ade her fe]\n",
            "[her feel v]  -->  [er feel ve]\n",
            "[eel very s]  -->  [el very sl]\n",
            "[ery sleepy]  -->  [ry sleepy ]\n",
            "[leepy and ]  -->  [eepy and s]\n",
            "[ and stupi]  -->  [and stupid]\n",
            "[stupid whe]  -->  [tupid whet]\n",
            "[d whether ]  -->  [ whether t]\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Display for y shift and  step_size\n",
        "def display_batch_generation(arr, char_to_int, int_to_char):\n",
        "    batch_size, seq_length, step_size = 8, 10, 5  # Setting step_size for overlap between sequences\n",
        "\n",
        "    x_batches, y_batches = get_batches(arr, batch_size, seq_length, step_size)\n",
        "\n",
        "    # Display batch number 10\n",
        "    x_chars = ''.join([int_to_char[idx] for idx in x_batches[10][0]])\n",
        "    y_chars = ''.join([int_to_char[idx] for idx in y_batches[10][0]])\n",
        "\n",
        "    print('='*50)\n",
        "    print('Displaying a Single Batch')\n",
        "    print('='*50)\n",
        "    for i in range(batch_size):\n",
        "        x_chars = ''.join([int_to_char[idx] for idx in x_batches[10][i]])\n",
        "        y_chars = ''.join([int_to_char[idx] for idx in y_batches[10][i]])\n",
        "\n",
        "        print(f\"[{x_chars}]  -->  [{y_chars}]\")\n",
        "    print('='*50)\n",
        "display_batch_generation(encoded_chars, char_to_int, int_to_char )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44411273-7fba-410d-9e99-1970a9450b6e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "44411273-7fba-410d-9e99-1970a9450b6e",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9edd3e101a0a07bdd165bfb9e12dfef4",
          "grade": true,
          "grade_id": "cell-df15e17decdee55f",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b5662b3c-84f4-402e-a503-ffb1930fdeaa",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b5662b3c-84f4-402e-a503-ffb1930fdeaa",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b407aa7e91d942d51744bd8f60bb70cd",
          "grade": false,
          "grade_id": "cell-63d39ee7de5e409f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 4. Define the Character-Level LSTM Model (6 points)\n",
        "In this step, you will implement the CharLSTM class, which processes sequences of characters and predicts the next character in the sequence. The model will learn sequential patterns in the data and store information over time using hidden states.\n",
        "\n",
        "##### Key Components:\n",
        "##### 1. Single Multi-Layer LSTM (see [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)):\n",
        "- Use a single `nn.LSTM` module configured as a multi-layer LSTM by setting the `num_layers` parameter to specify the number of stacked layers within this LSTM.\n",
        "- Dropout set in `nn.LSTM` automatically applies dropout between the internal LSTM layers (e.g., between the 1st and 2nd layers if num_layers=2). This dropout is only applied between the internal LSTM layers and does not affect the final output layer.\n",
        "\n",
        "##### 2. Dropout Layer (see [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)):\n",
        "- Define an additional dropout layer to be applied after the final LSTM layer. This helps to prevent overfitting.\n",
        "\n",
        "##### 3. Fully Connected Layer (see [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)):\n",
        "- After the LSTM layers, a fully connected layer maps the hidden states to process them as a probability distribution over the vocabulary to predict the next character in the sequence.\n",
        "\n",
        "##### 4. Hidden State Initialization:\n",
        "- The LSTM's hidden and cell states will be initialized with zero values before each batch is processed and will be updated as the model processes the sequence.\n",
        "\n",
        "##### Steps to follow:\n",
        "- You need to implement the following methods in the `CharLSTM` class:\n",
        "   - `__init__()` to define the architecture.\n",
        "   - `forward()` to handle the forward pass.\n",
        "   - `init_hidden()` to initialize the hidden and cell states before each batch.\n",
        "- The model architecture should follow this flow:\n",
        "    - One `nn.LSTM`: Define a single multilayer LSTM with `num_layers=num_layers`, output dimensionality `hidden_dim`, and internal dropout of `dropout_prob` between stacked layers.\n",
        "    - Dropout after LSTM: Apply a separate dropout layer `nn.Dropout` with the probability of `dropout_prob` to the LSTM’s output after all layers.\n",
        "    - Fully Connected Layer: Define a fully connected layer to output logits for each character in the sequence.\n",
        "\n",
        "**Important Notes**:\n",
        "- Ensure that you use the provided parameters (e.g., num_layers, hidden_dim) when defining the model architecture. Avoid hardcoding values (like `num_layers=2`)\n",
        "- One-hot encoding will be applied in the training loop before the data is passed to the model. This will adjust the shape of x for each batch to `(batch_size, seq_length, input_dim)`, where input_dim is the vocabulary size.  You do not need to handle this encoding within the CharLSTM class.\n",
        "- We will use cross-entropy loss as the loss function. The cross-entropy loss combines the softmax operation and the negative log-likelihood loss in a single step. The loss function takes the raw outputs (logits) from the fully connected layer and internally converts them to a probability distribution. Therefore, you do not need to apply Softmax separately.\n",
        "- Ensure the hidden states are initialized on the same device as the model parameters to avoid device mismatch errors.\n",
        "- Make sure to configure the LSTM as one `nn.LSTM` module with `num_layers` rather than separate layers. This approach is critical for testing.\n",
        "- Do not confuse stacked LSTM (configured with `num_layers`) with bidirectional LSTM.\n",
        "- Carefully check LSTM parameters and make sure that input and output shapes are correct. Pay special attention to the difference between batched and unbatched input shapes.\n",
        "- Consider how `batch_first` parameter of `nn.LSTM` aligns with the shape of your data after one-hot encoding and think about how the batch dimension should be treated within the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "96e05ab1-1dc9-42cb-9e2a-aeb226861f5a",
      "metadata": {
        "deletable": false,
        "id": "96e05ab1-1dc9-42cb-9e2a-aeb226861f5a",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f38a50dd45898543c6efc37941ce9ce0",
          "grade": false,
          "grade_id": "cell-f08538e1fc0fcaff",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-Level Multi-Layer LSTM Model\n",
        "\n",
        "    This model processes sequences of characters and predicts the next character in the sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, dropout_prob):\n",
        "        \"\"\"\n",
        "        Initializes the CharLSTM model with the specified parameters.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): Number of LSTM layers\n",
        "            input_dim (int): Dimensionality of the input (vocab size for one-hot)\n",
        "            hidden_dim (int): Dimensionality of the LSTM hidden layer.\n",
        "            output_dim (int): Dimensionality of the output (should equal vocab size)\n",
        "            dropout_prob (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super(CharLSTM, self).__init__()\n",
        "\n",
        "        # Save for init_hidden\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Single multi-layer LSTM\n",
        "        # batch_first=True → input shape: (batch_size, seq_length, input_dim)\n",
        "        # dropout=dropout_prob → applied between internal LSTM layers (not after last)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_prob  # internal dropout between stacked layers\n",
        "        )\n",
        "\n",
        "        # Separate dropout applied AFTER the final LSTM layer output\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        # Final fully-connected layer: maps hidden state → vocab logits\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: (batch_size, seq_length, input_dim) — one-hot encoded\n",
        "            hidden: tuple (h0, c0), each of shape (num_layers, batch_size, hidden_dim)\n",
        "\n",
        "        Returns:\n",
        "            out: (batch_size, seq_length, output_dim) — raw logits\n",
        "            (h_n, c_n): updated hidden and cell states\n",
        "        \"\"\"\n",
        "        # lstm_out: (batch_size, seq_length, hidden_dim)\n",
        "        # hidden states: h_n, c_n → each (num_layers, batch_size, hidden_dim)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x, hidden)\n",
        "\n",
        "        # Apply dropout to the entire sequence output\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Apply linear layer to every time step\n",
        "        # Contiguous() ensures correct memory layout after dropout\n",
        "        out = self.fc(lstm_out.contiguous())\n",
        "\n",
        "        # out shape: (batch_size, seq_length, output_dim)\n",
        "        return out, (h_n, c_n)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        Initializes hidden and cell states to zeros.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Current batch size\n",
        "\n",
        "        Returns:\n",
        "            (h0, c0): tuple of zeros tensors of shape (num_layers, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
        "\n",
        "        return (h0, c0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1f6df6dc-1e82-4ff9-bb5a-51bf8ecefff5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ff160bbe154aae64d238c9d351db531",
          "grade": true,
          "grade_id": "cell-dc352b07a1b59f67",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f6df6dc-1e82-4ff9-bb5a-51bf8ecefff5",
        "outputId": "824815ee-4fc6-4fed-c62d-abce72692b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout test passed successfully!\n"
          ]
        }
      ],
      "source": [
        "def test_dropout_effect():\n",
        "    model = CharLSTM(num_layers=2, input_dim=10, hidden_dim=100, output_dim=40, dropout_prob=0.1)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    hidden = model.init_hidden(64)\n",
        "    input_seq = torch.rand(64, 50, 10).to(device)\n",
        "\n",
        "    all_tests_successful = True\n",
        "    errors = []\n",
        "\n",
        "    model.train()\n",
        "    output1, _ = model(input_seq, hidden)\n",
        "    output2, _ = model(input_seq, hidden)\n",
        "    try:\n",
        "        assert not torch.equal(output1, output2), 'Dropout has no effect in training mode.'\n",
        "    except AssertionError as e:\n",
        "        errors.append(str(e))\n",
        "        all_tests_successful = False\n",
        "\n",
        "    model.eval()\n",
        "    output3, _ = model(input_seq, hidden)\n",
        "    output4, _ = model(input_seq, hidden)\n",
        "    try:\n",
        "        assert torch.equal(output3, output4), 'Outputs should be consistent in evaluation mode.'\n",
        "    except AssertionError as e:\n",
        "        errors.append(str(e))\n",
        "        all_tests_successful = False\n",
        "\n",
        "    if all_tests_successful:\n",
        "        print('Dropout test passed successfully!')\n",
        "    else:\n",
        "        feedback_txt.append('Task 4: Visible test: -1 points from the following:')\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError('\\n'.join(errors))\n",
        "\n",
        "test_dropout_effect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "284eda66-c2b5-4cff-847e-c4e4053759c0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3591ce2e786a68ae6aa3fc6cab6725d0",
          "grade": true,
          "grade_id": "cell-996ff5ec08dd085e",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "284eda66-c2b5-4cff-847e-c4e4053759c0",
        "outputId": "2d6bb495-1208-461e-d98b-71c53b72d1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM shape test passed successfully!\n"
          ]
        }
      ],
      "source": [
        "### BEGIN VISIBLE TESTS\n",
        "def test_lstm_model():\n",
        "    model = CharLSTM(num_layers=2, input_dim=10, hidden_dim=100, output_dim=40, dropout_prob=0.1)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    hidden = model.init_hidden(64)\n",
        "    input_seq = torch.rand(64, 50, 10).to(device)\n",
        "    all_tests_successful = True\n",
        "    errors = []\n",
        "\n",
        "    try:\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "\n",
        "        try:\n",
        "            assert output.shape == (64, 50, 40), f\"Expected output shape (64, 50, 40), but got {output.shape}.\"\n",
        "        except AssertionError as e:\n",
        "            errors.append(str(e))\n",
        "            all_tests_successful = False\n",
        "\n",
        "        (h, c) = hidden\n",
        "        try:\n",
        "            assert h.shape == (2, 64, 100), f\"Expected h shape (2, 64, 100), but got {h.shape}.\"\n",
        "        except AssertionError as e:\n",
        "            errors.append(str(e))\n",
        "            all_tests_successful = False\n",
        "\n",
        "        try:\n",
        "            assert c.shape == (2, 64, 100), f\"Expected c shape (2, 64, 100), but got {c.shape}.\"\n",
        "        except AssertionError as e:\n",
        "            errors.append(str(e))\n",
        "            all_tests_successful = False\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        errors.append(f\"RuntimeError: Check if batch_first=True is set properly. Details: {e}\")\n",
        "        all_tests_successful = False\n",
        "\n",
        "    if all_tests_successful:\n",
        "        print(\"LSTM shape test passed successfully!\")\n",
        "    else:\n",
        "        feedback_txt.append('Task 4: Visible test: -2 points from the following:')\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError('\\n'.join(errors))\n",
        "\n",
        "test_lstm_model()\n",
        "### END VISIBLE TESTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a39de60-9189-4ada-ac79-7c52058339c9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8ca734e34eb3f3bbfada1f88f6658be3",
          "grade": true,
          "grade_id": "cell-e621e79392118caa",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2a39de60-9189-4ada-ac79-7c52058339c9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bf5c3c04-852b-4fea-a96d-4440f1eca9f4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bf5c3c04-852b-4fea-a96d-4440f1eca9f4",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "04813018cf23bc6f0b88aecbd7e1257c",
          "grade": false,
          "grade_id": "cell-f811f0b42ac678b1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### 5. Train the Model (4 points)\n",
        "\n",
        "In this task, you will implement the training loop for the CharLSTM model.\n",
        "\n",
        "In the train() function you are given:\n",
        "- Optimizer and loss function: The [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) are used. Do not change the optimizer and loss function.\n",
        "- Preparing the Batches: The `get_batches()` function is used to divide the encoded text data into smaller input (x) and target (y) sequences.\n",
        "\n",
        "##### Steps to Follow:\n",
        "Outside of the iteration, initialize the hidden states using the `init_hidden()` method.\n",
        "\n",
        "For each iteration:\n",
        "##### 1. Encoding the input sequence:\n",
        "- For each batch, convert the input to one-hot representations (see [F.one_hot](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html))\n",
        "\n",
        "##### 2. Detach Hidden States:\n",
        "- Detach the hidden states after each batch to avoid backpropagating through previous batches to ensure efficient training.\n",
        "\n",
        "##### 3. Forward Pass and Loss Calculation:\n",
        "- For each batch, perform forward pass by passing the input x to the model.\n",
        "- The model will output logits, which will represent the predicted probabilities for the next character in the sequence when passed to the cross-entropy loss.\n",
        "- Use cross-entropy loss to compare the predicted output to the target y and calculate the error for the current batch.\n",
        "\n",
        "##### 4. Backpropagation and Parameter Update:\n",
        "- After calculating the loss compute the gradients with backward pass.\n",
        "- Update the model parameters using the optimizer.\n",
        "\n",
        "\n",
        "**Hints**:\n",
        "- Do not forget to zero the gradients.\n",
        "- Ensure that the logits and target tensors are reshaped appropriately to match the expected size for the loss function specified  [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). \\\n",
        "Logits should have shape (batch_size * seq_length, vocab_size) and targets should have shape (batch_size * seq_length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "85869fef-cd2c-4f5f-a34f-21efd2995817",
      "metadata": {
        "deletable": false,
        "id": "85869fef-cd2c-4f5f-a34f-21efd2995817",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c3bbd7de1795376183497192a382d6b9",
          "grade": false,
          "grade_id": "cell-12ea36dbb7f387ab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from unittest.mock import patch, MagicMock\n",
        "from functools import partialmethod\n",
        "\n",
        "def train(model, encoded_chars, vocab_size, num_epochs, batch_size,\n",
        "          seq_length, step_size, learning_rate, save_path=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Train the CharLSTM model on encoded text data.\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Prepare batches once (outside the epoch loop)\n",
        "    x_batches, y_batches = get_batches(encoded_chars, batch_size, seq_length, step_size)\n",
        "    num_batches = len(x_batches)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Initialize hidden state at the start of each epoch\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        # Progress bar\n",
        "        batch_loader = tqdm(zip(x_batches, y_batches), total=num_batches,\n",
        "                            leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        for x_batch, y_batch in batch_loader:\n",
        "            # Move data to device and convert to long (for one_hot)\n",
        "            x = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
        "            y = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
        "\n",
        "            # One-hot encode the input: (batch_size, seq_length) → (batch_size, seq_length, vocab_size)\n",
        "            x_one_hot = F.one_hot(x, num_classes=vocab_size).float()  # shape: (B, L, V)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass: get predictions and new hidden state\n",
        "            logits, hidden = model(x_one_hot, hidden)\n",
        "\n",
        "            # Detach hidden states to prevent backprop through entire history\n",
        "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "            # Reshape for CrossEntropyLoss: (B, L, V) → (B * L, V)\n",
        "            logits = logits.view(-1, vocab_size)   # (batch_size * seq_length, vocab_size)\n",
        "            targets = y.view(-1)                   # (batch_size * seq_length)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Backward pass and optimization step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            batch_loader.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        if verbose:\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Save model after each epoch\n",
        "        if save_path:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f'Model saved to {save_path} after epoch {epoch+1}')\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "11d395cf-1f84-4861-b62e-b8cc339cba21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "11d395cf-1f84-4861-b62e-b8cc339cba21",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "80cc427f4a4d11f886c9f2ffe0344a53",
          "grade": false,
          "grade_id": "cell-27ff7890904aba0c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "fc8dd53f-e043-4c38-893c-a2b576d510c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed: model.forward() was called successfully during training!\n"
          ]
        }
      ],
      "source": [
        "def test_model_forward_called():\n",
        "\n",
        "    vocab_size, hidden_dim, dropout_prob = 50, 12, 0.2\n",
        "    batch_size, seq_length, num_epochs = 2, 3, 1\n",
        "    test_chars = np.arange(vocab_size)\n",
        "    model = CharLSTM(2, vocab_size, hidden_dim, vocab_size, dropout_prob=dropout_prob)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    with patch.object(model, 'forward', wraps=model.forward) as mock_forward, \\\n",
        "         patch('torch.optim.Adam'), \\\n",
        "         patch('torch.nn.CrossEntropyLoss'), \\\n",
        "         patch('tqdm.tqdm.__init__', partialmethod(tqdm.__init__, disable=True)):\n",
        "\n",
        "        try:\n",
        "            # Run the train function\n",
        "            train(model, test_chars, vocab_size, num_epochs=num_epochs, batch_size=batch_size, seq_length=seq_length, step_size=seq_length, learning_rate=0.001, verbose=False)\n",
        "        except Exception as e:\n",
        "            feedback_txt.append(f\"Runtime: Exception occurred during training - {str(e)}\")\n",
        "\n",
        "        # Check if forward() was called\n",
        "        if not mock_forward.called:\n",
        "            feedback_txt.append(\"Task 5: Visible test: [model_forward_called] Training check: Expected model.forward() to be called at least once, but it was not.\")\n",
        "        else:\n",
        "            print(\"Test passed: model.forward() was called successfully during training!\")\n",
        "\n",
        "\n",
        "test_model_forward_called()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "076ce995-9c03-44ec-a343-9b8b66bd62fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "076ce995-9c03-44ec-a343-9b8b66bd62fb",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2819a902bda78a02b450da6061c78c5d",
          "grade": true,
          "grade_id": "cell-5fbdbaead214e1cd",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "2ca23893-13c8-4332-c55c-b838877b87e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed: Input tensor x has correct shape during training!\n"
          ]
        }
      ],
      "source": [
        "def test_input_shape():\n",
        "\n",
        "    vocab_size, hidden_dim, dropout_prob = 50, 12, 0.2\n",
        "    batch_size, seq_length, num_epochs = 2, 3, 1\n",
        "    test_chars = np.arange(vocab_size)\n",
        "    model = CharLSTM(2, vocab_size, hidden_dim, vocab_size, dropout_prob=0.1)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    errors = []\n",
        "\n",
        "    def forward_spy(x, hidden):\n",
        "        # Check input shape correctness\n",
        "        if x.shape != (batch_size, seq_length, vocab_size):\n",
        "            errors.append(f\"Input shape mismatch: Expected {(batch_size, seq_length, vocab_size)}, but got {x.shape}\")\n",
        "        return model.__class__.forward(model, x, hidden)\n",
        "\n",
        "    with patch.object(model, 'forward', wraps=forward_spy), \\\n",
        "         patch('torch.optim.Adam'), \\\n",
        "         patch('torch.nn.CrossEntropyLoss'), \\\n",
        "         patch('tqdm.tqdm.__init__', partialmethod(tqdm.__init__, disable=True)):\n",
        "\n",
        "        try:\n",
        "            train(model, test_chars, vocab_size, num_epochs=num_epochs, batch_size=batch_size, seq_length=seq_length, step_size=seq_length, learning_rate=0.001, verbose=False)\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Task 5: Visible test: Runtime: Exception occurred during training - {str(e)}\")\n",
        "\n",
        "    if errors:\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError(\" Errors found in input shape check.\")\n",
        "\n",
        "    print(\"Test passed: Input tensor x has correct shape during training!\")\n",
        "\n",
        "test_input_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7a3ae284-f4ae-421a-9e95-800648242bef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "7a3ae284-f4ae-421a-9e95-800648242bef",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a28ddf417561892e1b440bd76dd6e887",
          "grade": true,
          "grade_id": "cell-1707c1e471a1c52c",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "d619f5f9-f4f2-43ec-dbc0-1aa3934acbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed: Hidden states are properly detached from computation graph!\n"
          ]
        }
      ],
      "source": [
        "def test_hidden_state_requires_grad():\n",
        "\n",
        "    vocab_size, hidden_dim, dropout_prob = 50, 12, 0.2\n",
        "    batch_size, seq_length, num_epochs = 2, 3, 1\n",
        "    test_chars = np.arange(vocab_size)\n",
        "    model = CharLSTM(2, vocab_size, hidden_dim, vocab_size, dropout_prob=dropout_prob)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    errors = []\n",
        "\n",
        "    def forward_spy(x, hidden):\n",
        "        h, c = hidden\n",
        "        if h.requires_grad:\n",
        "            errors.append(\"Hidden state h should be detached (requires_grad=False).\")\n",
        "        if c.requires_grad:\n",
        "            errors.append(\"Hidden state c should be detached (requires_grad=False).\")\n",
        "        return model.__class__.forward(model, x, hidden)\n",
        "\n",
        "    with patch.object(model, 'forward', wraps=forward_spy), \\\n",
        "         patch('torch.optim.Adam'), \\\n",
        "         patch('torch.nn.CrossEntropyLoss'), \\\n",
        "         patch('tqdm.tqdm.__init__', partialmethod(tqdm.__init__, disable=True)):\n",
        "\n",
        "        try:\n",
        "            train(model, test_chars, vocab_size, num_epochs=num_epochs, batch_size=batch_size, seq_length=seq_length, step_size=seq_length, learning_rate=0.001, verbose=False)\n",
        "        except Exception as e:\n",
        "            errors.append(f\" Task 5: Visible test: Runtime: Exception occurred during training - {str(e)}\")\n",
        "\n",
        "    if errors:\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError(\"Hidden state detachment errors detected.\")\n",
        "\n",
        "    print(\"Test passed: Hidden states are properly detached from computation graph!\")\n",
        "\n",
        "test_hidden_state_requires_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7659e6b6-c73c-492b-8207-5a3d48ba63a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "7659e6b6-c73c-492b-8207-5a3d48ba63a8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "90289aee2fb08f5187bd4d9f3db4e5b5",
          "grade": true,
          "grade_id": "cell-579cf854d3ccaf48",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "73f23473-1e77-46e6-86c7-537c3e967400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed: Criterion arguments have expected shapes!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2501763096.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
            "/tmp/ipython-input-2501763096.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y_batch, dtype=torch.long).to(device)\n"
          ]
        }
      ],
      "source": [
        "def test_criterion_argument_shapes():\n",
        "\n",
        "    vocab_size, hidden_dim, dropout_prob = 50, 12, 0.2\n",
        "    batch_size, seq_length, num_epochs = 2, 3, 1\n",
        "    test_chars = np.arange(vocab_size)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    mock_model = CharLSTM(2, vocab_size, hidden_dim, vocab_size, dropout_prob=0.1).to(device)\n",
        "    errors = []\n",
        "\n",
        "    with patch(\"tqdm.tqdm.__init__\", partialmethod(tqdm.__init__, disable=True)), \\\n",
        "         patch(\"torch.optim.Adam\"):\n",
        "\n",
        "        def criterion_side_effect(output, y):\n",
        "            if output.shape != (batch_size * seq_length, vocab_size):\n",
        "                errors.append(f\"Criterion argument mismatch: Expected model output shape ({batch_size * seq_length}, {vocab_size}), but got {output.shape}.\")\n",
        "            if y.shape != (batch_size * seq_length,):\n",
        "                errors.append(f\"Criterion argument mismatch: Expected target (y) shape ({batch_size * seq_length},), but got {y.shape}.\")\n",
        "            return torch.tensor(0.0, requires_grad=True, device=device)\n",
        "\n",
        "        with patch(\"torch.nn.CrossEntropyLoss\", return_value=criterion_side_effect), \\\n",
        "             patch(\"__main__.get_batches\", return_value=(\n",
        "                 torch.randint(0, 50, (11, 2, 3), device=device),\n",
        "                 torch.randint(0, 50, (11, 2, 3), device=device)\n",
        "             )):\n",
        "\n",
        "            try:\n",
        "                train(mock_model, test_chars, vocab_size, num_epochs=num_epochs, batch_size=batch_size, seq_length=seq_length, step_size=seq_length, learning_rate=0.001, verbose=False)\n",
        "            except Exception as e:\n",
        "                errors.append(f\" Task 5: Visible test: Runtime: Exception occurred during training - {str(e)}\")\n",
        "\n",
        "    if errors:\n",
        "        feedback_txt.extend(errors)\n",
        "        raise AssertionError(\"Criterion argument shape errors detected.\")\n",
        "\n",
        "    print(\"Test passed: Criterion arguments have expected shapes!\")\n",
        "\n",
        "test_criterion_argument_shapes()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f744f6c3-06a4-47a3-9c46-c2344c4e74ab",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f744f6c3-06a4-47a3-9c46-c2344c4e74ab",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "78ba413d7fe1f539b5f90fc147ac112a",
          "grade": false,
          "grade_id": "cell-50b02537e194d303",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Model Initialization\n",
        "\n",
        "We suggest using two LSTM layers (num_layers=2) with a hidden dimension of 400 and dropout of 0.1. In the following cells, you will find the recommended setup for model initialization and training. Feel free to experiment with different parameters. However, before submission, ensure that the parameters you set below match those of your trained model. This is essential for your code to run correctly in inference mode. Aim for a training loss of less than 0.99."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3b68e133-661b-4382-bd4e-b08d6046763e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "3b68e133-661b-4382-bd4e-b08d6046763e",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "011c42d8611231ab6fb2e68b52c437ee",
          "grade": false,
          "grade_id": "cell-b525be2767a548e0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "335e0c44-662e-431c-a17b-0c0f0d2a555b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharLSTM(\n",
            "  (lstm): LSTM(27, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=400, out_features=27, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "hidden_dim = 400\n",
        "dropout_prob=0.1\n",
        "num_layers=2\n",
        "vocab_size = len(char_to_int)\n",
        "model = CharLSTM(num_layers, vocab_size, hidden_dim, vocab_size, dropout_prob)\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c738f5ec-9833-495d-b3e1-c0b61d31dee6",
      "metadata": {
        "id": "c738f5ec-9833-495d-b3e1-c0b61d31dee6"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50 # Train for *at least* 50 epochs to meet the min loss of 0.99\n",
        "batch_size = 50\n",
        "seq_length=100\n",
        "step_size=100\n",
        "learning_rate=0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3b371bf3-bf59-4204-84ea-903fc8eeee9b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b5464fa40099ca9dcc93745f47a0421a",
          "grade": false,
          "grade_id": "cell-4f276ff242dea824",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b371bf3-bf59-4204-84ea-903fc8eeee9b",
        "outputId": "89f0eb77-faaa-4612-d47c-738c5c8c158b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 27/27 [00:01<00:00, 22.38it/s, loss=2.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Average Loss: 2.8956\n",
            "Model saved to best_model.pth after epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 27/27 [00:00<00:00, 33.38it/s, loss=2.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50, Average Loss: 2.8011\n",
            "Model saved to best_model.pth after epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 27/27 [00:00<00:00, 33.33it/s, loss=2.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50, Average Loss: 2.7217\n",
            "Model saved to best_model.pth after epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 27/27 [00:00<00:00, 33.11it/s, loss=2.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50, Average Loss: 2.4561\n",
            "Model saved to best_model.pth after epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 27/27 [00:00<00:00, 33.14it/s, loss=2.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Average Loss: 2.2760\n",
            "Model saved to best_model.pth after epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 27/27 [00:00<00:00, 33.27it/s, loss=2.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50, Average Loss: 2.1722\n",
            "Model saved to best_model.pth after epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 27/27 [00:00<00:00, 32.69it/s, loss=2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50, Average Loss: 2.0857\n",
            "Model saved to best_model.pth after epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 27/27 [00:00<00:00, 32.83it/s, loss=1.94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50, Average Loss: 2.0112\n",
            "Model saved to best_model.pth after epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 27/27 [00:00<00:00, 32.64it/s, loss=1.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50, Average Loss: 1.9445\n",
            "Model saved to best_model.pth after epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 27/27 [00:00<00:00, 32.48it/s, loss=1.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Average Loss: 1.8832\n",
            "Model saved to best_model.pth after epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 27/27 [00:00<00:00, 32.68it/s, loss=1.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50, Average Loss: 1.8253\n",
            "Model saved to best_model.pth after epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 27/27 [00:00<00:00, 32.12it/s, loss=1.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50, Average Loss: 1.7718\n",
            "Model saved to best_model.pth after epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 27/27 [00:00<00:00, 31.89it/s, loss=1.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50, Average Loss: 1.7201\n",
            "Model saved to best_model.pth after epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 27/27 [00:00<00:00, 32.15it/s, loss=1.63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50, Average Loss: 1.6714\n",
            "Model saved to best_model.pth after epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 27/27 [00:00<00:00, 31.89it/s, loss=1.61]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50, Average Loss: 1.6289\n",
            "Model saved to best_model.pth after epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 27/27 [00:00<00:00, 31.44it/s, loss=1.55]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50, Average Loss: 1.5860\n",
            "Model saved to best_model.pth after epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 27/27 [00:00<00:00, 31.78it/s, loss=1.53]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50, Average Loss: 1.5452\n",
            "Model saved to best_model.pth after epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 27/27 [00:00<00:00, 31.99it/s, loss=1.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50, Average Loss: 1.5088\n",
            "Model saved to best_model.pth after epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 27/27 [00:00<00:00, 31.28it/s, loss=1.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50, Average Loss: 1.4765\n",
            "Model saved to best_model.pth after epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 27/27 [00:00<00:00, 31.19it/s, loss=1.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50, Average Loss: 1.4453\n",
            "Model saved to best_model.pth after epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 27/27 [00:00<00:00, 31.21it/s, loss=1.41]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50, Average Loss: 1.4125\n",
            "Model saved to best_model.pth after epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 27/27 [00:00<00:00, 30.96it/s, loss=1.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50, Average Loss: 1.3845\n",
            "Model saved to best_model.pth after epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 27/27 [00:00<00:00, 31.40it/s, loss=1.37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50, Average Loss: 1.3582\n",
            "Model saved to best_model.pth after epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 27/27 [00:00<00:00, 30.68it/s, loss=1.34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50, Average Loss: 1.3350\n",
            "Model saved to best_model.pth after epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 27/27 [00:00<00:00, 30.75it/s, loss=1.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50, Average Loss: 1.3099\n",
            "Model saved to best_model.pth after epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 27/27 [00:00<00:00, 30.28it/s, loss=1.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50, Average Loss: 1.2867\n",
            "Model saved to best_model.pth after epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 27/27 [00:00<00:00, 30.80it/s, loss=1.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50, Average Loss: 1.2652\n",
            "Model saved to best_model.pth after epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 27/27 [00:00<00:00, 30.52it/s, loss=1.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50, Average Loss: 1.2426\n",
            "Model saved to best_model.pth after epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 27/27 [00:00<00:00, 29.90it/s, loss=1.24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50, Average Loss: 1.2185\n",
            "Model saved to best_model.pth after epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 27/27 [00:00<00:00, 30.49it/s, loss=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50, Average Loss: 1.1998\n",
            "Model saved to best_model.pth after epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 27/27 [00:00<00:00, 30.22it/s, loss=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50, Average Loss: 1.1785\n",
            "Model saved to best_model.pth after epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 27/27 [00:00<00:00, 30.53it/s, loss=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50, Average Loss: 1.1569\n",
            "Model saved to best_model.pth after epoch 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 27/27 [00:00<00:00, 30.18it/s, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50, Average Loss: 1.1368\n",
            "Model saved to best_model.pth after epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 27/27 [00:00<00:00, 30.21it/s, loss=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50, Average Loss: 1.1151\n",
            "Model saved to best_model.pth after epoch 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 27/27 [00:00<00:00, 30.29it/s, loss=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50, Average Loss: 1.0930\n",
            "Model saved to best_model.pth after epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 27/27 [00:00<00:00, 30.98it/s, loss=1.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50, Average Loss: 1.0692\n",
            "Model saved to best_model.pth after epoch 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 27/27 [00:00<00:00, 30.78it/s, loss=1.07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50, Average Loss: 1.0496\n",
            "Model saved to best_model.pth after epoch 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 27/27 [00:00<00:00, 30.24it/s, loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50, Average Loss: 1.0279\n",
            "Model saved to best_model.pth after epoch 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 27/27 [00:00<00:00, 30.73it/s, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50, Average Loss: 1.0071\n",
            "Model saved to best_model.pth after epoch 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 27/27 [00:00<00:00, 30.40it/s, loss=1.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50, Average Loss: 0.9888\n",
            "Model saved to best_model.pth after epoch 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 27/27 [00:00<00:00, 31.05it/s, loss=0.987]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50, Average Loss: 0.9680\n",
            "Model saved to best_model.pth after epoch 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 27/27 [00:00<00:00, 30.93it/s, loss=0.977]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50, Average Loss: 0.9495\n",
            "Model saved to best_model.pth after epoch 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 27/27 [00:00<00:00, 30.93it/s, loss=0.959]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50, Average Loss: 0.9331\n",
            "Model saved to best_model.pth after epoch 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 27/27 [00:00<00:00, 31.69it/s, loss=0.941]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50, Average Loss: 0.9135\n",
            "Model saved to best_model.pth after epoch 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 27/27 [00:00<00:00, 31.02it/s, loss=0.902]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50, Average Loss: 0.8925\n",
            "Model saved to best_model.pth after epoch 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 27/27 [00:00<00:00, 31.91it/s, loss=0.888]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50, Average Loss: 0.8700\n",
            "Model saved to best_model.pth after epoch 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 27/27 [00:00<00:00, 31.71it/s, loss=0.865]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50, Average Loss: 0.8512\n",
            "Model saved to best_model.pth after epoch 47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 27/27 [00:00<00:00, 31.56it/s, loss=0.833]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50, Average Loss: 0.8274\n",
            "Model saved to best_model.pth after epoch 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 27/27 [00:00<00:00, 31.96it/s, loss=0.814]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50, Average Loss: 0.8023\n",
            "Model saved to best_model.pth after epoch 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 27/27 [00:00<00:00, 32.34it/s, loss=0.773]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50, Average Loss: 0.7786\n",
            "Model saved to best_model.pth after epoch 50\n"
          ]
        }
      ],
      "source": [
        "if not skip_training:\n",
        "    loss = train(\n",
        "        model=model,\n",
        "        encoded_chars=encoded_chars,\n",
        "        vocab_size=vocab_size,\n",
        "        num_epochs=num_epochs,\n",
        "        batch_size=batch_size,\n",
        "        seq_length=seq_length,\n",
        "        step_size=step_size,\n",
        "        learning_rate=learning_rate,\n",
        "        save_path='best_model.pth'\n",
        "    )\n",
        "else:\n",
        "    model.load_state_dict(torch.load('best_model.pth', weights_only=False, map_location=device))\n",
        "    print('Loaded weights from your saved model successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96gXJbp7YPb4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "96gXJbp7YPb4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "44e6c604a0b15603b762a905130f9dd7",
          "grade": true,
          "grade_id": "cell-041c8cd5cd7aa992",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338fcf4c-4b13-466f-bb2a-9fc79d276910",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "338fcf4c-4b13-466f-bb2a-9fc79d276910",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5f4810bbcce1d9b751981d42d7cec2e3",
          "grade": false,
          "grade_id": "cell-2c5996453d5e58cb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Step 6: Text Generation (2 points)\n",
        "The `text_generation()` function will use a trained model to create new text based on a given starting string.\n",
        "\n",
        "In this process, the model becomes autoregressive by using its own predictions as inputs for the next steps. Starting with an initial string, the model produces characters one by one, feeding each newly generated character back into itself as input.\n",
        "\n",
        "\n",
        "1. Input start string:\n",
        "The function begins with a starting string, which is converted into a sequence of integers using the character-to-integer mapping.\n",
        "2. Generate text:\n",
        "The model will take this sequence as input, predict the next character, and add it to the text. This process is repeated for a specified number of characters `predict_len`.\n",
        "3. Output probabilities\n",
        "\n",
        "There are a few steps in the function for you to implement.\n",
        "\n",
        "##### Steps to follow:\n",
        "1. One-Hot encoding: Apply one-hot encoding to the input sequence as you did in the training loop.\n",
        "\n",
        "2. Forward pass: Feed the one-hot encoded input through the model to obtain the output logits and the updated hidden state.\n",
        "\n",
        "3. Extract the last output: You only need the output from the **last time step** to predict the next character. Slice the output and get the last element along the sequence dimension. If you did the training well, the generated text should mostly include meaningful words.\n",
        "\n",
        "4. Temperature scaling: To control the randomness in prediction, divide the output logits by the temperature parameter. The temperature should be in range (0,1]. Higher temperatures produce more random text, while lower temperatures produce more predictable results. You can observe the variations in the generated text by experimenting with different temperature values.\n",
        "\n",
        "You can try generating text after training for just one epoch to observe the model's initial behavior. Depending on your temperature setting, the generated text might be a repetition of a single character or random sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "af09b58d-db73-438c-8146-2d3de3c7258c",
      "metadata": {
        "deletable": false,
        "id": "af09b58d-db73-438c-8146-2d3de3c7258c",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f150ea2ef8cc1bf48f64175bcab64bd2",
          "grade": false,
          "grade_id": "cell-f6a72e59083bfbfb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_str, char_to_int, int_to_char, vocab_size, predict_len=100, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text using the trained model.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Encode the starting string\n",
        "    input_seq = [char_to_int[char] for char in start_str]\n",
        "    input_seq = torch.tensor(input_seq, dtype=torch.long).to(device).unsqueeze(0)  # (1, len)\n",
        "\n",
        "    # Initialize hidden state for batch_size=1\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    generated_text = start_str\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # First, process the entire start_str to \"warm up\" the hidden state\n",
        "        # (Important: we need correct hidden state before generating new chars)\n",
        "        x_one_hot = F.one_hot(input_seq, num_classes=vocab_size).float()  # (1, L, V)\n",
        "        _, hidden = model(x_one_hot, hidden)\n",
        "\n",
        "        # Now use only the last character of start_str as current input\n",
        "        current_input = input_seq[:, -1:].clone()  # (1, 1)\n",
        "\n",
        "        for _ in range(predict_len):\n",
        "            # One-hot encode current input character\n",
        "            x_one_hot = F.one_hot(current_input, num_classes=vocab_size).float()  # (1, 1, V)\n",
        "\n",
        "            # Forward pass: get logits for next character\n",
        "            logits, hidden = model(x_one_hot, hidden)\n",
        "\n",
        "            # Extract logits from the last (only) time step\n",
        "            logits = logits[:, -1, :]  # (1, vocab_size)\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probabilities = F.softmax(logits, dim=-1)  # (1, vocab_size)\n",
        "            probabilities = probabilities.squeeze(0).cpu().numpy()  # (vocab_size,)\n",
        "\n",
        "            # Sample next character index\n",
        "            next_char_index = np.random.choice(vocab_size, p=probabilities)\n",
        "\n",
        "            # Decode and append\n",
        "            next_char = int_to_char[next_char_index]\n",
        "            generated_text += next_char\n",
        "\n",
        "            # Prepare next input: feed the predicted character back\n",
        "            current_input = torch.tensor([[next_char_index]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "cad2f2ef-fd10-484e-9824-aa90c90b1884",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cad2f2ef-fd10-484e-9824-aa90c90b1884",
        "outputId": "5d86259c-ff6d-44ba-c332-cb4107f3655b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we re all and the march hare said to herself alice replied said the king the mock turtle s remark the three gardeners only and round the court with a sme thought and i m sure i mand and she had never eat of this side of the beginn with a cat latten the duchess and the same thing and the knave was thinking over the duchess s got to the queen was some of the e e evening beautiful beautiful soup beautiful beautiful soup chapter with a sigh she had poor little thing bottle stapt she did not like that word but she was too much for its meant with the time the next witness was the march hare things was the fan and gloves the flower wood the queen she went on said alice why the arches close behand it said alice i don t know what the rest of the hatter went on in a mouse that s the mock turtle s beginning as all the window as she could the gryphon the words down with said the gryphon it s all the lobsters and the mock turtle s confusion which she said the gryphon what alice did not like the dormouse shou\n"
          ]
        }
      ],
      "source": [
        "start_str = 'we re all '\n",
        "predict_len = 1000\n",
        "temperature = 0.5\n",
        "generated_text = generate_text(model,\n",
        "                               start_str,\n",
        "                               char_to_int,\n",
        "                               int_to_char,\n",
        "                               vocab_size,\n",
        "                               predict_len=predict_len,\n",
        "                               temperature=temperature)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "Xj6_AWdglnz_",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Xj6_AWdglnz_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "439227f42fdc4fa29eb38c2af610b3df",
          "grade": true,
          "grade_id": "cell-fadac8751e2beb11",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "604864a6-b39d-47f2-a69f-b376ae8219b4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a6e4c74314a3cd1709560dc06c01dbbd",
          "grade": true,
          "grade_id": "cell-8238a89b4d5ba985",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "604864a6-b39d-47f2-a69f-b376ae8219b4"
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "7e7597e6-c781-404a-a6c7-2fa412e4ed11",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7e7597e6-c781-404a-a6c7-2fa412e4ed11",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ab892d080fd598f878aeeaaff91142b7",
          "grade": true,
          "grade_id": "cell-e3481764f39a6e57",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0c0a811f-924b-4e9b-b0e7-f906258966e1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4807a1a2c784d88ef5552940adecef3c",
          "grade": true,
          "grade_id": "cell-c284a3bf0bca125e",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0c0a811f-924b-4e9b-b0e7-f906258966e1"
      },
      "outputs": [],
      "source": [
        "# Do not delete this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8yIvE4HELntG",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8yIvE4HELntG",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "12530d89d930d1ef97974e4a9d4176a2",
          "grade": false,
          "grade_id": "cell-f84300429e028cb5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**Closing remarks**:\n",
        "\n",
        "1. Consider experimenting with more complex architectures by adding additional LSTM layers or increasing the hidden dimension size. Keep in mind that even with GPU resources this can take a while.\n",
        "\n",
        "2. In this task, we used one-hot encoding to represent inputs. However, you can experiment with the `nn.Embedding` module in PyTorch, which creates better representations for input characters and may improve model performance.\n",
        "\n",
        "3. For more complex models, you do not need to remove special characters, like punctuation and new lines, during preprocessing. Keeping these characters is helpful especially if you want to generate text in different styles, such as Shakespearean sonnets, where line breaks and punctuation are important for preserving the text style.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}